{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Machine Learning: Assignment 1\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 1 is due Wednesday, September 25 11:59 pm. Late work will not be accepted as per the course policies (see the syllabus on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Acknowledge any use of an AI system such as ChatGPT or Copilot.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Lasso\n",
    " * Bias-variance decomposition\n",
    " * Mercer kernels\n",
    " * LOOCV for kernel smoothing and ridge regression\n",
    "\n",
    "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Roping variables with the lasso (15 points)\n",
    "\n",
    "In this exercise, we'll employ the lasso regression technique to identify key predictor variables from the diabetes dataset. This dataset has its origins in the National Institute of Diabetes and Digestive and Kidney Diseases. The primary goal is to predict, based on diagnostic measurements, whether or not a patient has diabetes.\n",
    "\n",
    "To provide context, the dataset focuses on a very specific population: all female patients, aged 21 and above, of Pima Indian descent.\n",
    "\n",
    "**Dataset Features**:\n",
    "\n",
    "* **Pregnancies**: Number of times pregnant \n",
    "* **Glucose**: Plasma glucose concentration after 2 hours in an oral glucose tolerance test\n",
    "* **BloodPressure**: Diastolic blood pressure (mm Hg)\n",
    "* **SkinThickness**: Thickness of triceps skin fold (mm)\n",
    "* **Insulin**: 2-hour serum insulin level (mu U/ml)\n",
    "* **BMI**: Body mass index (calculated as weight in kg divided by height in m squared)\n",
    "* **DiabetesPedigreeFunction**: A function that scores likelihood of diabetes based on family history\n",
    "* **Age**: Age in years\n",
    "* **Outcome**: The target variable, indicating presence (1) or absence (0) of diabetes\n",
    "\n",
    "**Your tasks are as follows**:\n",
    "\n",
    "1. **Plotting Lasso Paths**: Generate a visualization of the lasso regularization paths.\n",
    "2. **Identifying Key Predictors**: Determine which coefficients of \\( \\beta \\) are non-zero.\n",
    "3. **Estimating Coefficients**: Provide the best estimate for these non-zero coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import Lasso\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run the next cell to read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/YData123/sds365-fa23/main/assignments/assn1/diabetes.csv')\n",
    "X = np.array(df.iloc[:,:8])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "n, p = X.shape\n",
    "print(\"Number of rows: {}\".format(n))\n",
    "print(\"Number of columns: {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Lasso regularization paths\n",
    "\n",
    "Run the lasso and plot the regularization paths. You can use the `Lasso` class from the `sklearn.linear_model` package. Plot the parameter paths with the regularization level $\\lambda$ (`alpha` in the code) on the log-scale, as done in the lasso demo code from class. (As always, be sure to label your axes.)\n",
    "\n",
    "Show two plots, one where you run the lasso on the variables as given in the dataset, another where you standardize the variables to have mean zero and standard deviation one. Describe the differences in the regularization paths, and explain those differences.\n",
    "\n",
    "When the predictors are standardized, what order do they appear in the lasso fits? That is, as $\\lambda$ decreases from infinity to zero, what is the sequence of variables that enter the model with nonzero coefficients? Explain why this ordering may (or may not) make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Select, estimate, and predict \n",
    "\n",
    "The true model is linear, and only a subset $S \\subset \\{0,1,\\ldots, 49\\}$ of the 50 variables have non-zero coefficients $\\beta_j$. In this problem you should make three estimates: \n",
    "\n",
    "1. An estimate $\\hat S$ of $S$\n",
    "2. An estimate $\\hat \\beta_j$ for each $j\\in \\hat S$\n",
    "3. An estimate of the predictive risk ${\\mathbb E}(Y - X\\hat\\beta)^2$\n",
    "\n",
    "\n",
    "We are not specifying how you should construct these estimates. You should use your judgement, taste, and \n",
    "the tools provided from class. However, you must clearly explain and justify whatever approach that you use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (and Markdown if needed) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Risky business (10 points)\n",
    "\n",
    "In class [(and in these notes)](https://github.com/YData123/sds365-fa22/raw/main/notes/kernel-bias-variance.pdf) we sketched a proof that, when the regression function has two bounded derivatives,\n",
    " the bias and variance for kernel smoothing scale as\n",
    "\n",
    "$$ \\mbox{bias}^2 = O\\left(h^4\\right)$$\n",
    "$$ \\mbox{var} = O\\left(\\frac{1}{nh^p}\\right).$$\n",
    "\n",
    "Here $h$ is the bandwidth parameter, $n$ is the sample size, and $p$ is the number of predictor variables. These expressions are asymptotic, meaning that they apply as $n$ gets large and $h$ gets small.  In this problem your job is to reason about the implications of this bias-variance decomposition for prediction.\n",
    "\n",
    "*Note:* For this problem, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or you write them on paper and scan to insert as an image in the notebook; whichever you prefer.\n",
    "\n",
    "\n",
    "### 2.1 Selecting the optimal bandwidth\n",
    "\n",
    "Suppose that the bias and variance are such that \n",
    "\n",
    "$$ \\mbox{bias}^2(\\hat m(x))  \\leq c_1 h^4 $$\n",
    "$$ \\mbox{var}(\\hat m(x)) \\leq c_2 \\frac{1}{nh^p}.$$\n",
    "\n",
    "for two constants $c_1$ and $c_2$. Using these expressions and a little calculus, determine the optimal bandwidth $h$ to minimize the risk function \n",
    "\n",
    "$$R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2.$$\n",
    "\n",
    "Your answer should involve the constants $c_1, c_2$, and $n$ and $p$. Give a bound on the resulting risk using this bandwidth.\n",
    "\n",
    "\n",
    "### 2.2 Bandwith selection without tears\n",
    "\n",
    "Now, going back to the expressions $\\mbox{bias}^2 = O\\left(h^4\\right)$ and $ \\mbox{var} = O\\left(\\displaystyle\\frac{1}{nh^p}\\right)$, explain why the scaling of the optimal bandwidth (as a function of $n$ and $p$), must satisfy \n",
    "$\\mbox{bias}^2  \\approx \\mbox{var}$; that is, they must be of the same order as $h\\to 0$. Then, without using any calculus, use this argument to determine the optimal scaling of the bandwidth and the fastest rate at which the \n",
    "risk $R(h) = {\\mathbb E}\\left(\\hat m(x) - m(x)\\right)^2$ can approach zero as the sample size increases.\n",
    "\n",
    "\n",
    "### 2.3 The cursed COD\n",
    "\n",
    "Using the risk bound you derive above, make a plot that demonstrates the curse of dimensionality by plotting the sample size required to achieve a given level of risk. Specifically, let the target risk $R$ vary between 0.1 and 0.5, and let the dimension $p$ vary between 1 and 20, and plot the sample size required to achieve that risk. Give a single plot that shows the collection of curves for each dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown with derivations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: A kernel of truth (15 points)\n",
    "\n",
    "For problem you will implement nonparametric regression using Mercer kernels and penalization, in 1-dimension. This can be compared to regression using smoothing kernels. \n",
    "\n",
    "As discussed in lecture, nonparametric regression with Mercer kernels is based on the infinite dimensional ridge regression\n",
    "\n",
    "$$ \\hat m = \\mbox{argmin} \\| Y - m \\|^2 + \\lambda \\|m\\|_K^2$$\n",
    "\n",
    "By the representer theorem, this is equivalent to setting $\\hat m(x) = \\sum_{i=1}^n \\hat \\alpha_i K(X_i, x)$ and \n",
    "using the finite dimensional optimization\n",
    "\n",
    "$$ \\hat \\alpha = \\mbox{argmin} \\| Y - {\\mathbb K} \\alpha \\|^2 + \\lambda \\alpha^T {\\mathbb K} \\alpha$$\n",
    "\n",
    "###  3.1 Solve \n",
    "\n",
    "Derive a closed-form expression for the minimizer $\\hat\\alpha$. Show all of the steps in your derivation, \n",
    "and justify each step. (As above, you may either enter your answers in Markdown using $\\rm\\LaTeX$, or insert an image of your handwritten solution.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 Implement\n",
    "\n",
    "Next you will use your solution above and implement Mercer kernel regression. We give some starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines some \"helper functions\" for this exercise. You don't need to change any of this code.\n",
    "(If you do want to make changes, just describe what you did and why.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimate(x, f, fhat, X, y, sigma, lmbda, sleeptime=.01):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(x, f, color='red', linewidth=2, label='true function')\n",
    "    plt.plot(x, fhat, color='blue', linewidth=2, label='estimated function')\n",
    "    plt.scatter(X, y, color='black', alpha=.5, label='random sample')\n",
    "    plt.ylim(np.min(f)-4*sigma, np.max(f)+4*sigma)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('lambda: %.3g' % lmbda)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('estimated m(x)')\n",
    "    plt.show()\n",
    "    sleep(sleeptime)\n",
    "    \n",
    "def true_fn(x):\n",
    "    return 3*x**2\n",
    "\n",
    "def run_simulation(kernel, lmbdas, show_bias_variance=True):\n",
    "    min_x, max_x = -1, 1\n",
    "    x = np.linspace(min_x, max_x, 100)\n",
    "    f = true_fn(x)\n",
    "    sigma = .25\n",
    "    estimates = []\n",
    "    trials = 500\n",
    "\n",
    "    for lmbda in lmbdas:\n",
    "        estimates_lambda = []\n",
    "        for i in np.arange(trials):\n",
    "            X = np.sort(np.random.uniform(low=min_x, high=max_x, size=50))\n",
    "            fX = true_fn(X)\n",
    "            y = fX + sigma*np.random.normal(size=len(X))\n",
    "            fhat = mercer_kernel_regress(kernel, X, y, x, lmbda=lmbda)\n",
    "            if i % 50 == 0:\n",
    "                plot_estimate(x, f, fhat, X, y, sigma, lmbda)\n",
    "            estimates_lambda.append(fhat)\n",
    "        estimates.append(estimates_lambda)\n",
    "\n",
    "    if show_bias_variance == False:\n",
    "        return\n",
    "    \n",
    "    fhat = np.array(estimates)\n",
    "    sq_bias = np.zeros(len(lmbdas))\n",
    "    variance = np.zeros(len(lmbdas))\n",
    "\n",
    "    for i in np.arange(len(lmbdas)):\n",
    "        sq_bias[i] = np.mean((np.mean(fhat[i], axis=0) - f)**2)\n",
    "        variance[i] = np.mean(np.var(fhat[i], axis=0))\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(lmbdas, sq_bias, label='squared bias', linewidth=2)\n",
    "    plt.plot(lmbdas, variance, label='variance', linewidth=2)\n",
    "    plt.plot(lmbdas, sq_bias + variance, label='risk')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to implement Mercer kernel regression and run it on two \n",
    "different kernel functions. The two kernels could simply be the Gaussian kernel\n",
    "with two different bandwidths, or you might experiment with other kernels.\n",
    "\n",
    "The function `mercer_kernel_regress` takes a kernel, training data `X` and `y`, an array of values `x` to evaluate the function on, and a regularization parameter. You'll use your derivation above to \n",
    "determine the coefficients $\\alpha$. For some clues and suggestions on how to do the \n",
    "implementation, see our demo code for smoothing kernels. You need to do something very similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mercer_kernel_regress(kernel, X, y, x, lmbda):  \n",
    "    # your implementation here\n",
    "    _\n",
    "\n",
    "def kernel1(x,y):\n",
    "    # your implementation here\n",
    "    _\n",
    "\n",
    "    \n",
    "def kernel2(x,y):\n",
    "    # your implementation here\n",
    "    _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.3 Run two simulations and select regularization parameters\n",
    "\n",
    "Finally, using our starter code and your own implementation above, run two simulations, one \n",
    "using `kernel1` and the other using `kernel2`. After each simulation, select a regularization level from the bias-variance tradeoff, and then run a final simulation with that regularization level. In the following \n",
    "starter code, you only need to specify the sequence of regularization parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbdas = # define your sequence of lambdas\n",
    "run_simulation(kernel1, lmbdas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_hat = # set the optimal lambda\n",
    "run_simulation(kernel1, [lambda_hat], show_bias_variance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbdas = # define your sequence of lambdas\n",
    "run_simulation(kernel2, lmbdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: An algebraic simplification of LOOCV (15 points)\n",
    "\n",
    "Leave-One-Out Cross Validation (LOOCV) is a specific type of \n",
    "$K$-fold cross validation where $K$ equals the number of observations in the dataset. \n",
    "It works as follows for a training set with $n$ observations:\n",
    "\n",
    "1. A single observation is used as the validation set, \n",
    "    and the remaining $n-1$ observations serve as the training set.\n",
    "2. A model is trained on the $n-1$ observations and \n",
    "    validated on the single left-out observation.\n",
    "3. This process is repeated $n$ times, each time leaving out a different \n",
    "    observation as the validation set.\n",
    "4. The LOOCV error is then the average error across all $n$ trials.\n",
    "\n",
    "LOOCV is particularly useful because:\n",
    "- It utilizes almost all the data for training, \n",
    "    so it's less prone to high variance compared to other validation schemes.\n",
    "- Since each observation is tested exactly once, \n",
    "    LOOCV provides a very thorough out-of-sample testing mechanism.\n",
    "\n",
    "However, it can be computationally expensive because you have to fit the model $n$ times. \n",
    "    Luckily, for some models, there are algebraic simplifications available \n",
    "    that make it computationally efficient.\n",
    "    Expressing LOOCV in terms of the hat matrix allows for efficient \n",
    "    computation of the LOOCV error without the need to refit the model for \n",
    "    each left-out observation, making it a valuable tool for model evaluation.\n",
    "\n",
    "Recall that the LOOCV error can be expressed as:\n",
    "\n",
    "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_{-i} \\right)^2, $$\n",
    "\n",
    "where $\\hat{y}_{-i}$ represents the prediction for the $i^{th}$ observation \n",
    "when it's left out from the training process.\n",
    "In the following questions, you will be deriving an alternative expression \n",
    "of the LOOCV error for both kernel and ridge regression, following the hints below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LOOCV for kernel smoothing:\n",
    "\n",
    "For kernels, we know that the LOOCV error can be equivalently written as the following form:\n",
    "\n",
    "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat y_i}{1 - L_{ii}} \\right)^2, $$\n",
    "where $\\hat y_i$ is the predicted value from the model fit on all data, and \n",
    "$L_{ii}$ is the $i^{th}$ diagonal element of the hat matrix $L$.\n",
    "\n",
    "For kernel regression, we have\n",
    "$$ \\hat{y} = L y, $$\n",
    "where\n",
    "- $ \\hat{y} $ is the vector of predictions.\n",
    "- $ y $ is the observed response values.\n",
    "- $ L $ is the hat matrix and is defined by the kernel (for a given bandwidth).\n",
    "So, each diagonal element $ L_{ii} $ of the matrix $ L $ is defined as:\n",
    "$$ L_{ii} = \\frac{K\\left(x_i, x_i\\right)}{\\sum_{j=1}^{n} K\\left(x_i, x_j\\right)}, $$\n",
    "where\n",
    "- $ K $ is the kernel function.\n",
    "- $ x_i $ and $ x_j $ are the predictor values for observations $ i $ and $ j $, respectively.\n",
    "\n",
    "The diagonal elements $ L_{ii} $ give the \"leverage\" of each observation, which can be interpreted as the influence an observation has on its own prediction. \n",
    "\n",
    "Derive this alternative expression of the LOOCV error for kernel regression. That's to say, for kernel regression, prove that\n",
    "\n",
    "$$ y_i - \\hat{y}_{-i}  =  \\frac{y_i - \\hat y_i}{1 - L_{ii}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOOCV for ridge regression:\n",
    "\n",
    "Recall that for ridge regression, the prediction equation can be written as:\n",
    "\n",
    "$$ \\hat{y} = X(X^T X + \\lambda I)^{-1}X^T y, $$\n",
    "\n",
    "where:\n",
    "- $X$ is the design matrix, and each row $x_i$ represents the $i^{th}$ observation, such that $X=\\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\\right]$\n",
    "- $y$ is the response vector.\n",
    "- $\\lambda$ is the ridge parameter.\n",
    "- $I$ is an identity matrix.\n",
    "\n",
    "As in the kernel smoothing case, the LOOCV can be written as:\n",
    "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i-\\hat y_i}{1 - H_{ii}} \\right)^2, $$\n",
    "where the hat matrix is\n",
    "\n",
    "$$ H = X(X^T X + \\lambda I)^{-1} X^T $$\n",
    "\n",
    "In the following steps, we will derive this alternative expression of the LOOCV error for ridge regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Using the [Sherman-Morrison\n",
    "formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), a special case of the [Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity):\n",
    "$$\n",
    "(A - uv^T)^{-1} = A^{-1} + \\frac{A^{-1}uv^T A^{-1}}{1 - v^T A^{-1}u}.\n",
    "$$\n",
    "\n",
    "The design matrix when we leave out the $i^{th}$ observation is denoted as $X_{-i}$. Prove that \n",
    "\n",
    "$$ (X_{-i}^T X_{-i} + \\lambda I)^{-1} = (X^T X + \\lambda I)^{-1} + \\frac{(X^T X + \\lambda I)^{-1}x_i x_i^T (X^T X + \\lambda I)^{-1}}{1 - x_i^T (X^T X + \\lambda I)^{-1}x_i} $$\n",
    "\n",
    "Hint: What's the relationship between $X_{-i}^T X_{-i}$ and $X^T X $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2  Let $A = X^T X + \\lambda I$. Using previous formulas,  prove that $\\hat{\\beta}_{-i}$, which is derived from $ \\hat{y}_{-i} = x_i^T \\hat{\\beta}_{-i} $, can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{\\beta}_{-i} & = A^{-1} X^T y-A^{-1} x_i y_i+\\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1}x_i}\\left(X^T y-x_i y_i\\right)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Note that the $i^{th}$ diagnoal element of $H$ satifies $H_{ii} = x_i^T A^{-1}x_i $.  Using previous formulas, prove that \n",
    "\n",
    "\n",
    "$$  \\hat{\\beta}_{-i} - \\hat{\\beta}  = \\frac{A^{-1} x_i \\left( \\hat{y}_i - y_i\\right)}{1 - H_{ii}}$$\n",
    "Hint : What is $ A^{-1} X^T y$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Finally, using previous formulas, prove that in the kernel smoothing case, the LOOCV can be written as:\n",
    "$$ LOOCV = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i-\\hat y_i}{1 - H_{ii}} \\right)^2 $$\n",
    "That's to say, prove that\n",
    "\n",
    "$$ y_i - \\hat{y}_{-i} = \\frac{y_i-\\hat y_i}{1 - H_{ii}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4a7aa81d2787d79eab8f6fb8f7b5343089747e772b346faf833d5b60a4f70bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
