{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Assignment 2\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 2 is due Wednesday, October 9 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Convolutional neural networks\n",
    " * Gaussian processes\n",
    " * Double descent\n",
    "\n",
    "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: It's not a bug, it's a feature! (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will [\"open the black box\"](https://news.yale.edu/2018/12/10/why-take-ydata-because-data-science-shouldnt-be-black-box) and inspect the filters and feature maps learned by a convolutional neural network trained to classify handwritten digits, using the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing the filters\n",
    "\n",
    "To begin, we load the dataset with 60000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", name='conv1'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", name='conv2'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train_binary, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained and tested the model, let's look at the filters learned in the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_conv1 = model.get_layer(name='conv1').get_weights()[0]\n",
    "\n",
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(8):\n",
    "        f = filters_conv1[:, :, 0, 8*i+j]\n",
    "        axs[i, j].imshow(f[:, :], cmap='gray')\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title(8*i+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you see. Do (some of) the learned filters make sense to you?\n",
    "\n",
    "Hint: Many filters have been designed and widely applied in image processing. [Here](http://www.theobjects.com/dragonfly/dfhelp/3-5/Content/05_Image%20Processing/Edge%20Detection%20Filters.htm) are some examples of edge detection filters and their effect on the image. You can find the details about each filter by clicking the links at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizing the feature maps\n",
    "\n",
    "We can also look at the corresponding feature map for each filter. There are 32 kernels at the first convolutional layer, so there are 32 feature maps for each sample. feature_map_conv1 is a 4D matrix where the first dimension is the index of the sample and the last dimension is the index of the correpsonding filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer('conv1').output)\n",
    "feature_map_conv1 = conv1_layer_model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly draw 16 samples for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = random.sample(range(1, len(x_test)), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose two filters among all 32 filters from 2.1, and visualize their feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_n1 = #\n",
    "filter_n2 = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to modify the next code cells, just run the four cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(filters_conv1[:, :, 0, filter_n1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv1[sample_index[4*i+j], :, :, filter_n1], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(filters_conv1[:, :, 0, filter_n2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv1[sample_index[4*i+j], :, :, filter_n2], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on what you see in the feature maps.\n",
    "* How do they correspond to the original images?\n",
    "* How do they correspond to the filters?\n",
    "* Why might the feature maps be helpful for classifying digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fitting a logistic regression model on feature maps\n",
    "\n",
    "The features of the images are further summarized after the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer('conv2').output)\n",
    "feature_map_conv2 = conv2_layer_model(x_test)\n",
    "\n",
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ix=0\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(x_test[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(feature_map_conv2[sample_index[4*i+j], :, :, 0], cmap='gray')\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and test a logistic regression model to classify two digits of your choice (i.e. a binary classification) using the features maps at the second convolutional layer as the input. You may use logistic regression functions such as [LogisticRegression in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Use 80% of the data for training and 20% for test.\n",
    "\n",
    "* How many features are there in your input X? Show the derivation of this number based on the architecture of the convolutional neural network.\n",
    "\n",
    "* How is your logistic regression model related to the fully connected layer and softmax layer in the convolutional neural network?\n",
    "\n",
    "* What is the accuracy of your model? Is this expected, or surprising? \n",
    "\n",
    "* Comment on any other aspects of your findings that are interesting to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lr = np.reshape(feature_map_conv2,(np.shape(feature_map_conv2)[0],-1))\n",
    "y_lr = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: All that glitters (20 points)\n",
    "\n",
    "In this problem you will use Gaussian process regression to model the trends in gold medal performances of selected events in the summer Olympics. The objectives of this problem are for you to:\n",
    "\n",
    "* Gain experience with Gaussian processes, to better understand how they work\n",
    "* Explore how posterior inference depends on the properties of the prior mean and kernel\n",
    "* Use Bayesian inference to identify unusual events\n",
    "* Practice making your Python code modular and reusable\n",
    "\n",
    "For this problem, the only starter code we provide is to read in the data and extract \n",
    "one event. You may write any GP code that you choose to, but please do not use any \n",
    "package for Gaussian processes; your code should be \"np-complete\" (using only \n",
    "basic `numpy` methods). You are encouraged to start from the [GP demo code](https://ydata123.org/fa24/interml/calendar.html) used in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran the GP demo code from class on the marathon data, it generated the following plot:\n",
    "<img src=\"https://github.com/YData123/sds365-fa22/raw/main/assignments/assn2/marathon.jpg\" width=\"600\">\n",
    "\n",
    "Note several properties of this plot:\n",
    "* It shows the Bayesian confidence of the regression, as a shaded area. This is a 95% confidence band because it has width given by $\\pm 2 \\sqrt{V}$, where $V$ is the estimated variance. The variance increases at the right side, for future years.\n",
    "\n",
    "* The gold medal time for the 1904 marathon is outside of this confidence band. In fact, the 1904 marathon was an [unusual event](https://www.smithsonianmag.com/history/the-1904-olympic-marathon-may-have-been-the-strangest-ever-14910747/), and this is apparent from the model. \n",
    "\n",
    "* The plot shows the posterior mean, and also shows one random sample from the posterior distribution.\n",
    "\n",
    "Your task in this problem is generate such a plot for six different Olympic events by writing a function\n",
    "\n",
    "`def gp_olympic_event(year, result, kernel, mean, noise, event_name):\n",
    "    ...`\n",
    "    \n",
    " where the input variables are the following:\n",
    " \n",
    "* `year`: a numpy array of years (integers)\n",
    "* `result`: a numpy array of numerical results, for the gold medal performances in that event\n",
    "* `kernel`: a kernel function \n",
    "* `mean`: a mean function \n",
    "* `noise`: a single float for the variance of the noise, $\\sigma^2$\n",
    "* `event_name`: a string used to label the y-axis, for example \"marathon min/mile (men's event)\"\n",
    " \n",
    "Your function should compute the Gaussian process regression, and then display the resulting plot, analogous to the plot above for the men's marathon event.\n",
    "\n",
    "You will then process **six** of the events, three men's events and three women's events, and call your function to generate the corresponding six plots.\n",
    "\n",
    "For each event, you should create a markdown cell that describes the resulting model. Comment on such things as:\n",
    "\n",
    "* How you chose the kernel, mean, and noise.\n",
    "* Why the plot does or doesn't look satisfactory to you\n",
    "* If there are any events such as the 1904 marathon that are notable.\n",
    "* What happens to the posterior mean (for example during WWII) if there are gaps in the data\n",
    "\n",
    "Use your best judgement to describe your findings; post questions to EdD if things are unclear. And have fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "In the remainder of this problem description, we recall how we processed the marathon data, as an example. The following cell reads in the data and displays the collection of events that are included in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dat = pd.read_csv('https://raw.githubusercontent.com/YData123/sds365-sp22/main/demos/gaussian_processes/olympic_results.csv')\n",
    "events = set(np.array(dat['Event']))\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then process the time to compute the minutes per mile (without checking that the race was actually 26.2 miles!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marathon = dat[dat['Event'] == 'Marathon Men']\n",
    "marathon = marathon[marathon['Medal']=='G']\n",
    "marathon = marathon.sort_values('Year')\n",
    "time = np.array(marathon['Result'])\n",
    "mpm = []\n",
    "for tm in time:\n",
    "    t = np.array(tm.split(':'), dtype=float)\n",
    "    minutes_per_mile = (t[0]*60*60 + t[1]*60 + t[2])/(60*26.2)\n",
    "    mpm.append(minutes_per_mile)\n",
    "    \n",
    "marathon['Minutes per Mile'] = np.round(mpm,2)\n",
    "marathon = marathon.drop(columns=['Gender', 'Event'], axis=1)\n",
    "marathon.reset_index(drop=True, inplace=True)\n",
    "year = np.array(marathon['Year'])\n",
    "result = np.array(marathon['Minutes per Mile'])\n",
    "marathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your code and markdown following this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Double descent! (20 points)\n",
    "\n",
    "<a href=\"https://skitheworld.com/2018/12/wurtele-twins-appointed-order-canada/\"><img src=\"https://raw.githubusercontent.com/YData123/sds365-fa22/main/assignments/assn1/double.jpg\" width=\"200\" align=\"left\" style=\"margin:10px 30px 10px 0px\"></a>\n",
    "\n",
    "\n",
    "In this problem you will explore the \"double descent\" phenomenon that was recently \n",
    "discovered as a key principle underlying the performance of deep neural networks.\n",
    "The problem setup is a \"random features\" version of a 2-layer neural network. The weights in the first layer are random and fixed, and the weights in the second layer are estimated from data. As we increase the number of neurons in the hidden layer, the dimension $p$ of model increases. It's helpful to define the ratio $\\gamma = p/n$ of variables to sample points. If $\\gamma < 1$ then we want to use the OLS estimator, and if $\\gamma > 1$ we want to use the minimum norm estimator. \n",
    "<br>\n",
    "\n",
    "Your mission (should you choose to accept it), is\n",
    "\n",
    "1. Implement a function `OLS_or_minimum_norm` that computes the least squares solution when $\\gamma < 1$, and the minimum norm solution when $\\gamma > 1$. (When $\\gamma=1$ the estimator does not, in general, exist.)\n",
    "1. Run the main code we give you to average over many trials, and to compute and plot the estimated risk for a range of values of $\\gamma$. \n",
    "1. Next, extend the starter code so that you compute (estimates of) the squared-bias and variance of the models. To do this, note that you'll need access to the true regression function, which is provided. You may want to refer to the demo code for smoothing kernels as an example.\n",
    "1. Using your new code, extend the plotting function we provide so that you plot \n",
    "the squared-bias, variance, and risk together on the same plot. \n",
    "1. Finally, comment on the results, describing why it might make sense that the squared bias, variance, and risk have the given shapes that they do.\n",
    "1. Show that in the overparameterized regime $\\gamma > 1$, as $\\lambda \\to 0$, the ridge regression estimator converges to the minimum norm estimator.\n",
    "\n",
    "\n",
    "By doing this exercise you will solidify your understanding of the meaning of bias and variance, and also gain a better understanding of the \"double descent\" phenomenon for overparameterized neural networks, \n",
    "and their striking resistance to overfitting.\n",
    "\n",
    "We're available in OH to help with any issues you run into!\n",
    "\n",
    "If you have any interest in background reading on this topic (not expected or required), we recommend Hastie et al., [\"Surprises in high-dimensional ridgeless least squares regression\"](https://www.stat.cmu.edu/~ryantibs/papers/ridgeless.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "Implement the function `OLS_or_minimum_norm` that computes the OLS solution for $\\gamma < 1$, and the minimum norm solution for $\\gamma > 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_or_minimum_norm(X, y):\n",
    "    ## Your code here\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plotting function we provide. No need to change this, although you can if you'd like.\n",
    "\n",
    "def plot_double_descent_risk(gammas, risk, sigma):\n",
    "    gammas = np.round(gammas, 2)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    tick_pos = np.zeros(len(gammas))\n",
    "    for i in np.arange(len(gammas)):\n",
    "        if gammas[i] <= 1:\n",
    "            tick_pos[i] = gammas[i] * 10\n",
    "        else:\n",
    "            tick_pos[i] = gammas[i] + 9\n",
    "    ax.axvline(x=tick_pos[np.array(gammas)==1][0], linestyle='dashed', color='gray')\n",
    "    ax.axhline(y=sigma**2, linestyle='dashed', color='gray')\n",
    "    ax.scatter(tick_pos, risk, color='salmon')\n",
    "    ax.plot(tick_pos, risk, color='gray', linewidth=.5)\n",
    "\n",
    "    tickgam = [gam for gam in gammas if (gam > .05 and gam <= .9) or gam >= 2 or gam == 1]\n",
    "    ticks = [tick_pos[j] for j in np.arange(len(tick_pos)) if gammas[j] in tickgam]\n",
    "    ax.xaxis.set_ticks(ticks)\n",
    "    ax.xaxis.set_ticklabels(tickgam)\n",
    "\n",
    "    plt.xlabel(r'$\\gamma = \\frac{p}{n}$', fontsize=18)\n",
    "    _ = plt.ylabel('Risk', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup \n",
    "\n",
    "The following cell sets up our data. The inputs $X$ are \n",
    "random Gaussian vectors of dimension $d=10$. Then, we map these \n",
    "using a neural network with fixed, Gaussian weights, to get random features\n",
    "corresponding to $p^* = 150$ hidden neurons. The second layer \n",
    "coefficients are $\\beta^* \\in {\\mathbb R}^{p^*}$, which are fixed. \n",
    "This defines the true model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just execute this cell, after you define the function above.\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "sigma = 1\n",
    "d = 10\n",
    "p_star = 150\n",
    "signal_size = 5\n",
    "\n",
    "W_star = (1/np.sqrt(d)) * np.random.randn(d, p_star)\n",
    "beta_star = np.arange(p_star)\n",
    "beta_star = signal_size * beta_star / np.sqrt(np.sum(beta_star**2))\n",
    "\n",
    "N = 10000\n",
    "X = np.random.randn(N, d)\n",
    "\n",
    "# f_star is the true regression function, for computing the squared bias\n",
    "f_star = np.dot(np.tanh(np.dot(X, W_star)), beta_star)\n",
    "noise = sigma * np.random.randn(N)\n",
    "y = f_star + noise\n",
    "yf = np.concatenate((y.reshape(N,1), f_star.reshape(N,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a sequence of models for different values of $\\gamma$\n",
    "\n",
    "Next, we train a sequence of models for different values of $\\gamma$, always \n",
    "fixing the sample size at $n=200$, but varying the dimension $p = \\gamma n$. \n",
    "When $p < p^*$ we just take the first $p$ features in the true model. \n",
    "When $p > p^*$ we add $p-p^*$ neurons to the hidden layer, with their \n",
    "own random weights.\n",
    "\n",
    "In the code below, we loop over the different values of $\\gamma$, \n",
    "and for each $\\gamma$ we run $100$ trials, each time generating \n",
    "a new training set of size $n=200$. The model (either OLS or minimum norm) is then computed, the MSE is computed, and finally the risk is estimated by averaging over all $100$ trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "n = 200\n",
    "\n",
    "gammas = list(np.arange(.1, 1, .1)) + [.92, .94, 1, 1.1, 1.2, 1.4, 1.6] + list(np.arange(2, 11, 1))\n",
    "gammas = [.01, .05] + gammas\n",
    "risk = []\n",
    "for gamma in gammas:\n",
    "    err = []\n",
    "    p = int(n * gamma)\n",
    "    if gamma == 1:\n",
    "        risk.append(np.inf)\n",
    "        continue\n",
    "    W = (1/np.sqrt(d)) * np.random.randn(d, p)\n",
    "    W[:,:min(p, p_star)] = W_star[:,:min(p, p_star)]\n",
    "    for i in np.arange(trials):\n",
    "        X_train, X_test, yf_train, yf_test = train_test_split(X, yf, train_size=n, test_size=1000)\n",
    "        H_train = np.tanh(np.dot(X_train, W))\n",
    "        H_test = np.tanh(np.dot(X_test, W))\n",
    "        beta_hat = OLS_or_minimum_norm(H_train, yf_train[:,0])\n",
    "        yhat_test = H_test @ beta_hat \n",
    "        err.append(np.mean((yhat_test - yf_test[:,0])**2))\n",
    "    print('gamma=%.2f  p=%d  n=%d  risk=%.3f' % (gamma, p, n, np.mean(err)))\n",
    "    risk.append(np.mean(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the risk\n",
    "\n",
    "At this point, you can plot the risk by just evaluating the cell below. \n",
    "This should reveal the \"double descent\" behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just evaluate the next line \n",
    "plot_double_descent_risk(gammas, risk, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2\n",
    "\n",
    "Comment on the results. Explain why the risk plot does or does not make sense \n",
    "in each regime: The underparameterized regime $\\gamma < 1$, and the overparameterized \n",
    "regime $\\gamma > 1$. Is the curve \"U-shaped\" in the underparameterized regime? Why or why not?\n",
    "What about in the overparameterized regime? You will be able to give better answers to these questions when you estimate the bias and variance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3\n",
    "\n",
    "Now, modify the above code so that you can estimate both the squared bias and the \n",
    "variance of the estimator. Before you do this, you may want to revisit the kernel smoothing demo from class, where we computed the squared bias, variance, and risk. You'll need the true function, which is provided in the variable `yf`.  You should not have to write a lot of code, but can compute the bias and variance after you store the predicted values on the test data for each trial.\n",
    "\n",
    "Plot the results, by plotting both the squared bias, the variance, and the risk for the sequence of gammas. To do this you will have to modify the plotting function appropriately, but this again involves minimal changes. When you obtain your final plot, comment \n",
    "on the shape of the bias and variance curves, as above for Problem 3.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and markdown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4\n",
    "\n",
    "In class, we discussed the interpretation of the minimum-norm estimator $ \\hat{\\beta}_{\\text{mn}} $. Geometrically, we can describe $ \\hat{\\beta}_{\\text{mn}} $ as the orthogonal projection of the zero vector in $ \\mathbb{R}^p $ onto the $ (p-1) $-dimensional hyperplane $ \\{ \\beta : X\\beta = Y \\} $.  \n",
    "\n",
    "This can also be viewed as \"ridgeless\" regression. In ridge regression, we minimize the objective function\n",
    "$$\n",
    "\\| Y - X\\beta  \\|_2^2 + \\lambda \\| \\beta \\|_2^2,\n",
    "$$\n",
    "which has the closed-form solution\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T Y.\n",
    "$$\n",
    "\n",
    "In the overparameterized regime where $ p > n $, it can be shown that as $ \\lambda \\to 0 $, $ \\hat{\\beta}_{\\lambda} $ converges to $ \\hat{\\beta}_{\\text{mn}} $. \n",
    "\n",
    "Your task is to show that as $ \\lambda \\to 0 $, the limit of the ridge regression estimator $ \\hat{\\beta}_{\\lambda} $, in the overparameterized regime where $ \\gamma > 1 $, is the minimum-norm estimator $ \\hat{\\beta}_{\\text{mn}} $. You may want to use the Woodbury formula for this derivation.\n",
    "\n",
    "\n",
    "_Hint_:\n",
    "1. Applying the simplified version of Woodbury formula\n",
    "$$\n",
    "(I + UV^T)^{-1} = I - U(I + V^T U)^{-1} V^T.\n",
    "$$ \n",
    "we can derive the identity:\n",
    "$$\n",
    "(X^T X + \\lambda I_p)^{-1} X^T = X^T (X X^T + \\lambda I_n)^{-1},\n",
    "$$\n",
    "2. You might consider using the Woodbury formula twice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4a7aa81d2787d79eab8f6fb8f7b5343089747e772b346faf833d5b60a4f70bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
