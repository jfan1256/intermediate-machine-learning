{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532ce372",
   "metadata": {
    "id": "5ccb1d8a"
   },
   "source": [
    "# Intermediate Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 5 is due Wednesday, December 6 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * RNNs and GRUs\n",
    " * Transformers\n",
    "\n",
    "This assignment will also help to solidify your Python skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5796",
   "metadata": {
    "id": "78f64b66"
   },
   "source": [
    "## Problem 1: Elephants Can Remember (25 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d390776",
   "metadata": {
    "id": "ba3663cd"
   },
   "source": [
    "![ECR](https://upload.wikimedia.org/wikipedia/en/e/e3/Elephants_can_Remember_First_Edition_Cover_1972.jpg) \n",
    "\n",
    "In this problem, we will work with \"vanilla\" Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book [Elephants Can Remember](https://en.wikipedia.org/wiki/Elephants_Can_Remember) by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available through Canvas, under the file name \"problem1_data\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4808eef9",
   "metadata": {
    "id": "0804721c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import random\n",
    "  \n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936afdde",
   "metadata": {
    "id": "f93de082"
   },
   "outputs": [],
   "source": [
    "with open('problem1_data/Agatha_Christie_train.txt', 'r') as file:\n",
    "    train_text = file.read()\n",
    "    \n",
    "with open('problem1_data/Agatha_Christie_test.txt', 'r') as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "vocabulary = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Dictionaries to go from a character to index and vice versa\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd892ac9",
   "metadata": {
    "id": "7320ed64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrs. oliver looked at herself in the glass. she gave a brief, sideways look towards the clock on the mantelpiece, which she had some idea was twenty minutes slow. then she resumed her study of her coiffure. the trouble with mrs. oliver was--and she admitted it freely--that her styles of hairdressing were always being changed. she had tried almost everything in turn. a severe pompadour at one time, then a wind-swept style where you brushed back your locks to display an intellectual brow, at least'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 500 characters of our training set\n",
    "train_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a2fed8",
   "metadata": {
    "id": "3a50abac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 46 characters\n",
      "The training set contains 262436 characters\n",
      "The test set contains 7209 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
    "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
    "print(\"The test set contains\", len(test_text) ,\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4d85f",
   "metadata": {
    "id": "d02ce399"
   },
   "source": [
    "### Problem 1.1: The Diversity of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a0f68",
   "metadata": {
    "id": "6951a417"
   },
   "source": [
    "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d0eb6",
   "metadata": {
    "id": "472741ad"
   },
   "source": [
    "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec87e68-b741-41bf-9b6b-226f8dfc9e1f",
   "metadata": {},
   "source": [
    "### Answer to 1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e316e1f-e68e-4562-8784-33576ab9737a",
   "metadata": {
    "id": "06832930"
   },
   "source": [
    "A Vanilla RNN computes the hidden state $h_t$ by combining the input at time $t$ with the previous hidden state. In theory, $h_t$ should carry long-term information, but in practice, it often fails due to vanishing or exploding gradients (i.e., in backpropagation you repeatedly take the derivative w.r.t to $h_t$, without consider it's the contribution at timestep $t$ due to recurrence). \n",
    "\n",
    "GRUs address these issues using two gates: the update gate ($\\gamma^u$) and the reset gate ($\\gamma^r$). The update gate controls how much past information is preserved, mitigating vanishing gradients by directly passing information when $\\gamma^u=1$ (i.e., in backpropagation the gradient is not depedent on $\\gamma^u$ instead of $h_t$). The reset gate determines how much past information is ignored, allowing the network to reset memory selectively. These mechanisms enable GRUs to capture both short- and long-term dependencies effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d7520",
   "metadata": {
    "id": "61c45e86"
   },
   "source": [
    "2. Describe at least two advantages of a character based language model over a word based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8c7eb-e665-4813-8a99-151461098d70",
   "metadata": {},
   "source": [
    "### Answer to 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a507969-c341-4dbb-b5f9-82cfeec416f6",
   "metadata": {
    "id": "8b07db8a"
   },
   "source": [
    "1. Character-based models process sequences at the character level, avoiding the out-of-vocabulary issue in word-based models by recognizing all possible inputs.\n",
    "2. Character-based models capture subword patterns (e.g., prefixes, suffixes), enabling better handling of rare or unseen words and morphologically rich languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362b6d2",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Problem 1.2: Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce4312",
   "metadata": {
    "id": "85bdca8b"
   },
   "source": [
    "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "847dc334",
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"problem1_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "022b37e2",
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Vanilla_RNN_1 (SimpleRNN)   (None, 100, 128)          22144     \n",
      "                                                                 \n",
      " Vanilla_RNN_2 (SimpleRNN)   (None, 64)                12352     \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 44)                2860      \n",
      "                                                                 \n",
      " Softmax_layer (Activation)  (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,356\n",
      "Trainable params: 37,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e3160",
   "metadata": {
    "id": "a4bcccd8"
   },
   "source": [
    "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c2408bb",
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Your code starts here\n",
    "    output_sentence = prompt\n",
    "    \n",
    "    # Process the prompt to initialize hidden states\n",
    "    for char in prompt:\n",
    "        # Convert the character to one-hot encoding\n",
    "        # x = np.zeros((len(char_to_indices), 1))\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "\n",
    "        # Calculate h1 and h2\n",
    "        h1 = np.tanh(np.dot(W_h1h1, h1) + np.dot(W_xh1, x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2, h2) + np.dot(W_h1h2, h1) + b_h2)\n",
    "    \n",
    "    # Convert last character in prompt to one-hot encode\n",
    "    # x = np.zeros((len(char_to_indices), 1))\n",
    "    x = np.zeros((44, 1))\n",
    "    x[char_to_indices[prompt[-1]]] = 1\n",
    "    \n",
    "    # Predict next character for N - len(prompt) steps\n",
    "    for _ in range(N - len(prompt)):\n",
    "        # Calculate h1 and h2\n",
    "        h1 = np.tanh(np.dot(W_h1h1, h1) + np.dot(W_xh1, x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2, h2) + np.dot(W_h1h2, h1) + b_h2)\n",
    "        \n",
    "        # Calculate probability\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        prob = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n",
    "\n",
    "        # Sample next character\n",
    "        next_char_index = np.random.choice(len(prob), p=prob.ravel())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        # Add character to output_sentence\n",
    "        output_sentence += next_char\n",
    "        \n",
    "        # Update last character\n",
    "        # x = np.zeros((len(char_to_indices), 1))\n",
    "        x = np.zeros((44, 1))\n",
    "        x[next_char_index] = 1\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb8ad8-ca13-41c8-be13-e2fd0084ebf5",
   "metadata": {},
   "source": [
    "### NOTE: Without changing the code or files, the length of vocabulary size is 46, whereas the model's vocabulary size is 44. Thus, this create a dimension mismatch when I do _x = np.zeros((len(char_to_indices), 1))_. I am not sure if this is a problem on my end or the homework provider, so to fix it I just hardcoded dimension 44 in _x = np.zeros((44, 1))_. The last two characters of the vocab are \"”\" and \"€\", so I believe this is okay. I will do this going forward for all the implementation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c7d2119-b72f-44cf-93c5-fb572e79e3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '&': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '/': 10,\n",
       " ':': 11,\n",
       " ';': 12,\n",
       " '?': 13,\n",
       " '[': 14,\n",
       " ']': 15,\n",
       " '^': 16,\n",
       " 'a': 17,\n",
       " 'b': 18,\n",
       " 'c': 19,\n",
       " 'd': 20,\n",
       " 'e': 21,\n",
       " 'f': 22,\n",
       " 'g': 23,\n",
       " 'h': 24,\n",
       " 'i': 25,\n",
       " 'j': 26,\n",
       " 'k': 27,\n",
       " 'l': 28,\n",
       " 'm': 29,\n",
       " 'n': 30,\n",
       " 'o': 31,\n",
       " 'p': 32,\n",
       " 'q': 33,\n",
       " 'r': 34,\n",
       " 's': 35,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 38,\n",
       " 'w': 39,\n",
       " 'x': 40,\n",
       " 'y': 41,\n",
       " 'z': 42,\n",
       " 'â': 43,\n",
       " '”': 44,\n",
       " '€': 45}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reference as to what I see\n",
    "char_to_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12109704",
   "metadata": {
    "id": "cfc7420f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8daafb8",
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. any fain, he tearser the netmen there will, mostirould deled me. i exvecaptins. chited lejprothere coustny dran it a mather, it's not a lo know. the ladion sad was fir. \"works it.\" \"i'm auned, seapes mother they nate that.\" \"oh, yes. take there is that time of har very one of these, ceeln and wrot the idealh,\" something i have year of wherced schaat that to ravery wanlw. \"mel the beas.\" \"a rimot? spersure. was manough nothing. chised saint knew why feels, where gyon's ked sometins. om somewhy recedser, as easind. ohith, came malted about say tok so ist. the litthim poirut.\" but it the course, mething well, you was what like afforcliably aritticully a liston'he tiding.\" \"abseally was him to it agsionle's quee to have it if and steeny right.\" \"whing dain was bean,\" happengething. there, \"whell you, what had try, i very dod was stauts for did you my oad grsoindion what sis in coment aling about he. but sort ot anestion chilln, and come have poir\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass.', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb226c",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Problem 1.3: Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc4860",
   "metadata": {
    "id": "1dd93ee7"
   },
   "source": [
    "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "493c9c05",
   "metadata": {
    "id": "82291909"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"problem1_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9557ce05",
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 512)               857088    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 44)                22572     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 879,660\n",
      "Trainable params: 879,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80319480",
   "metadata": {
    "id": "cad5b34f"
   },
   "source": [
    "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
    "\n",
    "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af754987",
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Your code starts here\n",
    "    output_sentence = prompt\n",
    "    \n",
    "    # Process the prompt to initialize the hidden state\n",
    "    for char in prompt:\n",
    "        # Convert the character to one-hot encoding\n",
    "        # x = np.zeros((len(char_to_indices), 1))\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "\n",
    "        # Calculate GRU gates and update hidden state\n",
    "        gamma_u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        gamma_r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.multiply(gamma_r, np.dot(W_hh, h)) + b_h)\n",
    "        h = np.multiply((1 - gamma_u), c_t) + np.multiply(gamma_u, h)\n",
    "    \n",
    "    # Convert last character in prompt to one-hot encode\n",
    "    # x = np.zeros((len(char_to_indices), 1))\n",
    "    x = np.zeros((44, 1))\n",
    "    x[char_to_indices[prompt[-1]]] = 1\n",
    "    \n",
    "    # Predict next character for N - len(prompt) steps\n",
    "    for _ in range(N - len(prompt)):\n",
    "        # Calculate gamma_u and gamma_r\n",
    "        gamma_u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        gamma_r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        \n",
    "        # Calculate c_t\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.multiply(gamma_r, np.dot(W_hh, h)) + b_h)\n",
    "        \n",
    "        # Calculate h\n",
    "        h = np.multiply((1 - gamma_u), c_t) + np.multiply(gamma_u, h)\n",
    "        \n",
    "        # Calculate probability\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        prob = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n",
    "        \n",
    "        # Sample next character\n",
    "        next_char_index = np.random.choice(len(prob), p=prob.ravel())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        # Add character to output_sentence\n",
    "        output_sentence += next_char\n",
    "        \n",
    "        # Update last character\n",
    "        # x = np.zeros((len(char_to_indices), 1))\n",
    "        x = np.zeros((44, 1))\n",
    "        x[next_char_index] = 1\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c53c4",
   "metadata": {
    "id": "117fdc8f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fe03acfe",
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. mrs. oliver and was mrs. ouite ferent hose of it as the time or sometimes. there were ently chiphed at a ntwo later a domother because i was a tull to meen the matter to her or help it oition arouse, i see, i an for a time of a deal ar if a had and t an i was she when or or anyir the atet a time. and lod eestake and as a well which he all strying to arrest after the wome in one been inem the botite fith pelia. the home to were a has tot tee trous all that. and the pase be ars ather that she houpe tald might the dight te to have read your dother, a dod the siling to really a litela to came to loling it if you tell you at liase i rother tho get she woll the mother this it someit inti a can lott a morting the sors that shatisine a dryen tee head his atfiction and i think. that had well that i am so they thought peicoun the oot to see mr. eld wh theredea same tire well here a stros it mort, i notilote with see you they were to you?\" they were t a\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass.',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d782698",
   "metadata": {
    "id": "6a35ba68"
   },
   "source": [
    "### Problem 1.4: Can Elephants Remember Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca65ce",
   "metadata": {
    "id": "09cd85c4"
   },
   "source": [
    "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
    "$$\n",
    "\n",
    "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
    "\n",
    "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
    "\n",
    "$$\n",
    "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\} \n",
    "$$\n",
    "\n",
    "Don't forget to go back to the normal perplexity after this transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c378b",
   "metadata": {
    "id": "709c6d4e"
   },
   "source": [
    "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964ff28-a1ef-4b78-a94f-5b4093c1af06",
   "metadata": {},
   "source": [
    "### Answer to 1.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e157e2b-872d-4b66-99ff-fbf39223b487",
   "metadata": {
    "id": "3ccf8083"
   },
   "source": [
    "The generated text from the Vanilla RNN is less coherent, with frequent nonsensical words, awkward phrasing, and inconsistent grammar, such as \"necmulbedreats soors\" and \"you what octidnain.\" It struggles to maintain context and deviates from the prompt quickly, reflecting its difficulty in capturing long-term dependencies due to the vanishing gradient problem. In contrast, the GRU produces text that is more structured and contextually aligned with the prompt, maintaining better narrative flow. This improvement stems from the GRU's gating mechanisms, which effectively preserve relevant information and manage dependencies across long sequences. However, the GRU also suffers from nonsensical words, awkward phrasing, and inconsistent grammar, albeit occurring later in the passage due to its improved ability to contextualize over longer dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e2bfd",
   "metadata": {
    "id": "5c9a5bf5"
   },
   "source": [
    "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4531b69-ef79-4cd8-8d26-71a5ea9177b7",
   "metadata": {},
   "source": [
    "### Answer to 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0e1a325-1cf3-4cf7-9d8f-fa1628573608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplex_rnn(weights, prompt, test_sequence):\n",
    "    # Extract weights and biases\n",
    "    W_xh1 = weights[0].T\n",
    "    W_h1h1 = weights[1].T\n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initialize hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "\n",
    "    # Process the prompt to initialize hidden states\n",
    "    for char in prompt:\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "        h1 = np.tanh(np.dot(W_h1h1, h1) + np.dot(W_xh1, x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2, h2) + np.dot(W_h1h2, h1) + b_h2)\n",
    "\n",
    "    # Store log probabilities for the test sequence\n",
    "    log_prob_sum = 0\n",
    "    N = len(test_sequence)\n",
    "\n",
    "    # Iterate through the test sequence\n",
    "    for char in test_sequence:\n",
    "        # Compute hidden states\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "        h1 = np.tanh(np.dot(W_h1h1, h1) + np.dot(W_xh1, x) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h2h2, h2) + np.dot(W_h1h2, h1) + b_h2)\n",
    "\n",
    "        # Compute output probabilities\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        prob = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n",
    "\n",
    "        # Update the log probability sum\n",
    "        log_prob_sum += np.log(prob[char_to_indices[char]])\n",
    "\n",
    "    # Compute log perplexity\n",
    "    log_perplexity = -log_prob_sum / N\n",
    "\n",
    "    # Convert back to perplexity\n",
    "    perplexity = np.exp(log_perplexity)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca2494b9-3060-4fed-8638-ede4df892a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplex_gru(weights, prompt, test_sequence):\n",
    "    # Extract weights and biases\n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis=0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis=0)\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "\n",
    "    # Process the prompt to initialize the hidden state\n",
    "    for char in prompt:\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "        gamma_u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        gamma_r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.multiply(gamma_r, np.dot(W_hh, h)) + b_h)\n",
    "        h = np.multiply((1 - gamma_u), c_t) + np.multiply(gamma_u, h)\n",
    "\n",
    "    # Initialize log probability sum for the test sequence\n",
    "    log_prob_sum = 0\n",
    "    N = len(test_sequence)\n",
    "\n",
    "    # Process the test sequence\n",
    "    for char in test_sequence:\n",
    "        # Compute GRU gates and update hidden state\n",
    "        x = np.zeros((44, 1))\n",
    "        x[char_to_indices[char]] = 1\n",
    "        gamma_u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        gamma_r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.multiply(gamma_r, np.dot(W_hh, h)) + b_h)\n",
    "        h = np.multiply((1 - gamma_u), c_t) + np.multiply(gamma_u, h)\n",
    "\n",
    "        # Compute output probabilities\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        prob = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n",
    "\n",
    "        # Update the log probability sum\n",
    "        log_prob_sum += np.log(prob[char_to_indices[char]])\n",
    "\n",
    "    # Compute log perplexity\n",
    "    log_perplexity = -log_prob_sum / N\n",
    "\n",
    "    # Convert back to perplexity\n",
    "    perplexity = np.exp(log_perplexity)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2e8da78-db06-4a4d-85b7-41b1bee58f82",
   "metadata": {
    "id": "3ccf8083"
   },
   "outputs": [],
   "source": [
    "m = 500\n",
    "test_prompt = test_text[:m]\n",
    "test_sequence = test_text[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7f7500d8-fb49-41f5-9674-32559d4ebf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Perplexity: [13459.50108872]\n",
      "GRU Perplexity: [5419.42895791]\n"
     ]
    }
   ],
   "source": [
    "rnn_perplex = get_perplex_rnn(weights_RNN, test_prompt, test_sequence)\n",
    "print(\"RNN Perplexity:\", rnn_perplex)\n",
    "gru_perplex = get_perplex_gru(weights_GRU, test_prompt, test_sequence)\n",
    "print(\"GRU Perplexity:\", gru_perplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95d95cbb-9019-4a49-8b11-4fb26fbcdb06",
   "metadata": {
    "id": "3ccf8083"
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "test_prompt = test_text[:m]\n",
    "test_sequence = test_text[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4c5a88c-9176-4d1e-8f9f-253de4793a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Perplexity: [13397.71259237]\n",
      "GRU Perplexity: [5423.56328402]\n"
     ]
    }
   ],
   "source": [
    "rnn_perplex = get_perplex_rnn(weights_RNN, test_prompt, test_sequence)\n",
    "print(\"RNN Perplexity:\", rnn_perplex)\n",
    "gru_perplex = get_perplex_gru(weights_GRU, test_prompt, test_sequence)\n",
    "print(\"GRU Perplexity:\", gru_perplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6001ca8-1a42-4800-96ed-a5031f8b10d1",
   "metadata": {
    "id": "8df914d2"
   },
   "source": [
    "After experimenting with various prompts (starting characters), the GRU model consistently produced perplexity scores much lower than those of the vanilla RNN. This indicates that the GRU model is less \"surprised\" by the test text, demonstrating a better understanding of the underlying language patterns compared to the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659674b8-ba74-4c2f-b2cf-d18bc8a54f15",
   "metadata": {
    "id": "9ab728c9"
   },
   "source": [
    "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31392a6-0a6d-48ea-bd9c-e50cb8814e6c",
   "metadata": {},
   "source": [
    "### Answer to 1.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d43ba-2116-4c50-a019-e57fcbb815ab",
   "metadata": {
    "id": "8df914d2"
   },
   "source": [
    "Currently, the model generates text by relying solely on the hidden state of the immediately preceding word. This approach limits its ability to capture long-term dependencies and nuanced contextual relationships within the text. A significant improvement would involve incorporating the hidden states of all previous words, weighted dynamically based on their relevance to the current word being generated. This technique, known as attention, is a core mechanism used in transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b7fe6",
   "metadata": {},
   "source": [
    "## Problem 2: Be the Bard (30 points)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Shakespeare.jpg\" alt=\"William\" width=\"200\" style=\"float:left; padding:15px\"/>\n",
    "\n",
    "Transformer models are the current state of the art in many sequence modeling tasks, and the Transformer architecture underlies most Large Language Models (LLMs), including ChatGPT, Llama, Mistral, etc.\n",
    "\n",
    "In this problem, we will implement a Transformer language model from scratch in `numpy`. You will be provided with the weights of a small, slightly simplified Transformer language model that we trained on the works of Shakespeare. We will walk through implementing each component of the Transformer architecture and ultimately assemble this into a language model that can generate some text in the style of the Bard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93cca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#!pip install tiktoken\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8019aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "vocab_size = 1024\n",
    "block_size = 128\n",
    "\n",
    "transformer_model_weights = np.load(f'problem2_model_parameters/model_weights_D{d_model}L{n_layers}H{n_heads}.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0401772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "    word_embedding.weight\n",
      "    position_embedding.inv_freq\n",
      "    layers.0.attention.wq.weight\n",
      "    layers.0.attention.wk.weight\n",
      "    layers.0.attention.wv.weight\n",
      "    layers.0.attention.wo.weight\n",
      "    layers.0.feed_forward.0.weight\n",
      "    layers.0.feed_forward.2.weight\n",
      "    layers.1.attention.wq.weight\n",
      "    layers.1.attention.wk.weight\n",
      "    layers.1.attention.wv.weight\n",
      "    layers.1.attention.wo.weight\n",
      "    layers.1.feed_forward.0.weight\n",
      "    layers.1.feed_forward.2.weight\n",
      "    layers.2.attention.wq.weight\n",
      "    layers.2.attention.wk.weight\n",
      "    layers.2.attention.wv.weight\n",
      "    layers.2.attention.wo.weight\n",
      "    layers.2.feed_forward.0.weight\n",
      "    layers.2.feed_forward.2.weight\n",
      "    layers.3.attention.wq.weight\n",
      "    layers.3.attention.wk.weight\n",
      "    layers.3.attention.wv.weight\n",
      "    layers.3.attention.wo.weight\n",
      "    layers.3.feed_forward.0.weight\n",
      "    layers.3.feed_forward.2.weight\n",
      "    fc_out.weight\n",
      "    fc_out.bias\n"
     ]
    }
   ],
   "source": [
    "print('Parameters:')\n",
    "for key in transformer_model_weights.keys():\n",
    "    print(f'    {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c04f5",
   "metadata": {},
   "source": [
    "### Problem 2.1: Describe the model parameters\n",
    "\n",
    "Describe the role of the parameters of the model. What are the weights `wq.weight`, `wk.weight`, `wv.weight`, and `wo.weight`? What are the dimensions of these matrices? What about the role and dimensions of the feedforward weights `feed_forward.0.weight` and `feed_forward.2.weight`?\n",
    "\n",
    "You can refer to the descriptions below as well as lecture notes. Note, in particular, in the comments preceding Problem 2.4 that the attention matrices across heads are \"packed together\" when you read in the parameters in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66841e",
   "metadata": {},
   "source": [
    "### Answer to 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8598ca7-6477-45ef-aed1-4852beb77396",
   "metadata": {},
   "source": [
    "1. The parameters determine the embedding size, the number of attention heads, vocabulary size, and feed-forward size. \n",
    "1. wq.weight is the query weight matrix. This projects the input embeddings into the query vector space, which represents what the input \"wants to focus on\" in other tokens. It has dimensions (d_model, d_head x n_heads) where d_model = 512, d_head = d_model / n_heads = 64, and n_heads = 8. Thus, wq.weight.dim = (512, 512)\n",
    "1. wk.weight is the key weight matrix. This projects the input embedding into the key vector space, which are used to match with queries to determine attention scores. It has same dimensions as wq.weight.\n",
    "1. wv.weight is the value weight matrix. This projects the input embeddings into the value vector space, which represents the information to be aggregated based on the attention scores. It has same dimensions as wq.weight.\n",
    "1. wo.weight is the output weight matrix. This projects the concatenated outputs from all attention heads back into the model's input space. It has same dimensions as wq.weight.\n",
    "1. feed_forward.0.weight expands the input embeddings through a linear transformation followed by an activation function. This is used to introduce non-linearity to the model as well as capture complex behaviors. It has dimensions (d_ff, d_model) = (1024, 512)\n",
    "1. feed_forward.2.weight reduces the expanded embeddings through a linear transformation. This is used to easily feed into the next layer of the transformer. It has dimensions (d_model, d_ff) = (512, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c7b84",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To process text with a neural network model, we need to first tokenize it in order to convert it to a numerical format that the model can understand and process. The tokenizer converts strings into sequences of integer tokens in a fixed vocabulary. There are different ways to do this. For this problem, we trained a custom [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  (BPE) tokenizer on Shakespeare text, setting the vocabulary size to 1024. The code below demonstrates how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1705eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BPE tokenizer\n",
    "with open('problem2_model_parameters/bpe1024_enc_full.pkl', 'rb') as pickle_file:\n",
    "    enc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed255879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n",
      "\n",
      "Encoded:\n",
      "[462, 320, 307, 258, 813, 63, 323, 621, 331, 800, 258, 697, 305, 10, 889, 801, 845, 813, 504, 260, 109, 408, 366, 818, 59]\n",
      "\n",
      "Tokens: \n",
      "['What', \"'s\", ' in', ' a', ' name', '?', ' that', ' which', ' we', ' call', ' a', ' ro', 'se', '\\n', 'By', ' any', ' other', ' name', ' would', ' s', 'm', 'ell', ' as', ' sweet', ';']\n",
      "\n",
      "Decoded text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n"
     ]
    }
   ],
   "source": [
    "# text to tokenize\n",
    "text = \"\"\"What's in a name? that which we call a rose\n",
    "By any other name would smell as sweet;\"\"\"\n",
    "\n",
    "print('Original text:')\n",
    "print(text)\n",
    "encoded = enc.encode(text)\n",
    "print()\n",
    "\n",
    "print('Encoded:')\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Tokens: ')\n",
    "print([enc.decode([idx]) for idx in encoded])\n",
    "decoded = enc.decode(encoded)\n",
    "print()\n",
    "\n",
    "print('Decoded text:')\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536a8cd",
   "metadata": {},
   "source": [
    "### Problem 2.2a: Token Embeddings\n",
    "\n",
    "After tokenization, we get a sequence of integers that represent the text to processed, with each integer index corresponding to a particular token in the vocabulary (e.g., a word or word-part). The first step in processing this text is to turn it into a vector representation. This is done via a learned embedding look-up table. For each token in the vocabulary $t \\in \\mathcal{V}$, we learn an embedding $E_t \\in {\\mathbb R}^d$. A sequence of tokens $(t_1, ..., t_n)$ is transformed to a vector representation by mapping each token to its embedding $(E_{t_1}, ..., E_{t_n}) \\in {\\mathbb R}^{n \\times d}$. This sequence of vectors is what the neural network model ultimately operates over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d3c9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens, params):\n",
    "    \"\"\"\n",
    "    Embed tokens using the input embeddings.\n",
    "\n",
    "    Args:\n",
    "        tokens (np.array): array of token indices, shape (n_tokens,)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # get needed parameters\n",
    "    embeddings = params['word_embedding.weight'] # shape (vocab_size, d_model)\n",
    "    # embeddings is a look up table for embeddings, with rows corresponding to token indices\n",
    "    # i.e., embeddings[token_index] returns the embedding for the token with index token_index\n",
    "\n",
    "    # look up embeddings\n",
    "    embedded_tokens = np.array([embeddings[token] for token in tokens])\n",
    "    return embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8199da5-fb9c-415e-964a-1c4478678f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
       "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
       "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens(np.array([1, 2, 3]), params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b3e99",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
    "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
    "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
    "      dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4c27",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Transformer models are by-default permutation-equivariant. That is, they don't understand order or position. To make them understand positional information, we need to encode it directly in the token embeddings. One way to do this is to represent each possible position $i$ with its own position embedding ${PE}_i \\in \\mathbb{R}^{d}$. One way to encode the position of each token is to simply add a positional embedding representing the position of the token.\n",
    "\n",
    "In the original Transformer paper, the authors propose a particular choice for ${PE}_i \\in {\\mathbb R}^{d}$ based on sines and cosines with frequencies depending on the position $i$. Since the original proposal, many follow-up works proposed different positional encoding methods aiming to improve performance and length-generalization. In this problem, we'll use the sinusoidal positional encodings of the original Transformer paper.\n",
    "\n",
    "We provide the code for computing these sinusoidal positional embeddings below. To give some intuition about the structure of the sinusoidal positional embeddings, we also plot a heatmap of the pairwise inner products $\\langle PE_i, PE_j \\rangle$. We see that that positions that are closer together have more similar positional embeddings, with additional oscillatory behavior on top of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215faef",
   "metadata": {},
   "source": [
    "### Problem 2.2b: Describe the positional embeddings\n",
    "\n",
    "Below is an implementation of the positional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0f9aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embeddings(sequence_length, dim, base=10000):\n",
    "    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))\n",
    "    t = np.arange(sequence_length)\n",
    "    sinusoid_inp = np.einsum(\"i,j->ij\", t, inv_freq)\n",
    "\n",
    "    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n",
    "\n",
    "    emb = np.concatenate((sin, cos), axis=-1)\n",
    "    # return emb[None, :, :]\n",
    "    return emb[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4975426",
   "metadata": {},
   "source": [
    "Explore the positional embeddings by computing the inner-product between all pairs of positional embeddings, and then plotting the similarity matrix as a heat map. Do the results make sense? What are the positional embeddings designed to model? Do they do this effectively? Comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ddc46dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe = get_sinusoidal_positional_embeddings(128, 256)\n",
    "\n",
    "#Your code here\n",
    "pe_similarity = np.dot(pe, pe.T)\n",
    "\n",
    "sns.heatmap(pe_similarity)\n",
    "plt.title('Similarity Structure of Sinusoidal Positional Embeddings')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92850d0c",
   "metadata": {},
   "source": [
    "### Answer to 2.2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c65473-7a88-4a69-85e3-625a0901d175",
   "metadata": {},
   "source": [
    "The diagonal is bright because the inner product of a vector with itself is the highest. There are periodic rectangles (or waves) that accurately reflect the periodic behavior of sinusoidal waves. Similarity decays as positions become farther apart, demonstrating how positional differences are effectively encoded. The goal of these embeddings is to encode the position of tokens in a sequence, providing the transformer model with a way to understand positional information. This is achieved by assigning a different sinusoidal frequency to each token, ensuring unique and distinguishable embeddings for every position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc783c84",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The core operation in a Transformer is multi-head attention, sometimes called self-attention. In this problem, we will implement multi-head attention in numpy from scratch, given the trained parameters of the model.\n",
    "\n",
    "The input is a sequence of vectors $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$. For each attention head $h \\in [n_h]$, the parameters consist of a query projection matrix $W_q^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, a key projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, and a value projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$. Here, $d_h$ is the \"head dimension\", taken to be $d_{model} / n_h$ (to maintain the same dimensionality between the input and output). The algorithm, for each head, is the following:\n",
    "1. Compute the queries $Q = X W_q^h$, keys $K = X W_k^h$, and values $V = X W_v^h$. Note that the linear maps are applied independently for each token across the embedding dimension (not sequence dimension), such that $Q, K, V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "2. Compare the queries and keys via inner products to get an $n \\times n$ attention matrix $A = \\mathrm{Softmax}(Q K^{\\intercal} / \\sqrt{d_h}) \\in {\\mathbb R}^{n \\times n}$.\n",
    "3. Use the attention scores $A$ to select values, producing the output of the self-attention head: $\\mathrm{head}_h = A V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "We then concatenate the retrieved values across all heads, and apply a final linear map. Putting this all together yields:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathrm{head}_h &= \\mathrm{Softmax}((X W_q^h) (X W_k^h)^{\\intercal}/ \\sqrt{d_h}) X W_v^h\\\\\n",
    "    \\mathrm{MultiHeadAttention}(X) &= \\mathrm{concat}(\\mathrm{head}_1, ..., \\mathrm{head}_{n_h}) W_o\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Note that the matrices $W_q^h$, $W_k^h$, $W_v^h$ are \"packed together\" across heads when you read in the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf260",
   "metadata": {},
   "source": [
    "### Problem 2.3: Implement multi-head attention\n",
    "\n",
    "Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a33c8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we provide a couple of utility functions\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # a stable implementation of the softmax function\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def apply_presoftmax_causal_mask(attn_scores):\n",
    "    # apply a causal mask to the attention scores (set the entries above the diagonal to -inf)\n",
    "    n = attn_scores.shape[-1]\n",
    "    mask = np.triu(np.ones((n, n)), k=1)\n",
    "    masked_scores = attn_scores - 1e9 * mask\n",
    "    return masked_scores\n",
    "\n",
    "def multi_head_attention(x, params, layer_prefix='layers.0'):\n",
    "    \"\"\"\n",
    "    Compute multi-head self-attention.\n",
    "\n",
    "    Args:\n",
    "        x (np.array): input tensor, shape (n, d_model)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "        layer_prefix (str): prefix of parameter names corresponding to the layer\n",
    "        verbose (bool): whether to print intermediate shapes\n",
    "    \"\"\"\n",
    "\n",
    "    # get parameters of multi-head attention layer\n",
    "    wq = params[f'{layer_prefix}.attention.wq.weight'].T # (d_model, d_model)\n",
    "    wk = params[f'{layer_prefix}.attention.wk.weight'].T # (d_model, d_model)\n",
    "    wv = params[f'{layer_prefix}.attention.wv.weight'].T # (d_model, d_model)\n",
    "    wo = params[f'{layer_prefix}.attention.wo.weight'].T # (d_model, d_model)\n",
    "\n",
    "    head_dim = d_model // n_heads # dimension of each head\n",
    "    attn_scale = 1 / math.sqrt(head_dim) # scaling factor for attention scores\n",
    "\n",
    "    # the wq, wk, wv, wo matrices contain weights for all heads, concatenated\n",
    "    # first, we split wq, wk, wv, wo into heads\n",
    "    # note: there are more efficient implementations, but this is more verbose/pedagogical\n",
    "    wq = wq.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wk = wk.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wv = wv.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "\n",
    "    head_outputs = []\n",
    "    for head in range(n_heads):\n",
    "\n",
    "        # get head-specific parameters (these are the query/key/value projections for this head)\n",
    "        wqh = wq[head] # (d_model, head_dim)\n",
    "        wkh = wk[head] # (d_model, head_dim)\n",
    "        wvh = wv[head] # (d_model, head_dim)\n",
    "\n",
    "        # compute queries, keys, values\n",
    "        # your code here\n",
    "        q = np.dot(x, wqh) # (n, head_dim)\n",
    "        k = np.dot(x, wkh) # (n, head_dim)\n",
    "        v = np.dot(x, wvh) # (n, head_dim)\n",
    "\n",
    "        # compute attention scores\n",
    "        # your code here\n",
    "        attn_scores = np.dot(q, k.T) # (n, n)\n",
    "\n",
    "        attn_scores = apply_presoftmax_causal_mask(attn_scores)\n",
    "        attn_scores = softmax(attn_scores * attn_scale, axis=-1) # (n, n)\n",
    "\n",
    "        # apply attention scores to values\n",
    "        # your code here\n",
    "        head_out = np.dot(attn_scores, v)  # (n, head_dim)\n",
    "\n",
    "        # store the head output\n",
    "        head_outputs.append(head_out)\n",
    "\n",
    "    # concatenate all head outputs\n",
    "    head_outputs = np.concatenate(head_outputs, axis=-1) # (n, d_model)\n",
    "\n",
    "    # apply output linear map W_o to concatenated head outputs\n",
    "    # your code here\n",
    "    output = np.dot(head_outputs, wo)  # (n, d_model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f2811",
   "metadata": {},
   "source": [
    "### Problem 2.4: Test your Attention implementation\n",
    "\n",
    "To test if you have the correct implementation, you can run the following \n",
    "test line. We show the expected output if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f08934d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
       "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
       "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e60b",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262bb79",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Each Transformer layer (i.e., block) consists of two operations: 1) (multi-head) self-attention, which enables exchange of information between tokens, and 2) a multi-layer perceptron, which processes each token independently. A Transformer model is essentially just alternating between these two operations. In this problem, we will implement the multi-layer perceptron step. Typically, the MLP at each layer is simply a two-layer (one hidden layer) MLP or Feed Forward Network. In our model, we use a ReLU activation in the hidden layer, though other activations are possible. The same MLP network is applied to each token embedding in the sequence independently. \n",
    "\n",
    "Given $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$, we apply the MLP as follows:\n",
    "\n",
    "$$\\mathrm{MLP}(X) = \\mathrm{ReLU}(X W_1) W_2$$\n",
    "\n",
    "Note that we don't use biases for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e95c1",
   "metadata": {},
   "source": [
    "### Problem 2.5: Implement the MLP\n",
    "\n",
    "Next, we need to apply the multi-layer perceptron in each layer. Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bab44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def mlp(x, params, layer_prefix='layers.0'):\n",
    "    # get MLP parameters\n",
    "    w1 = params[f'{layer_prefix}.feed_forward.0.weight'].T # (d_model, d_ff)\n",
    "    w2 = params[f'{layer_prefix}.feed_forward.2.weight'].T # (d_ff, d_model)\n",
    "\n",
    "    # Your code here \n",
    "    h = relu(np.dot(x, w1))\n",
    "    o = np.dot(h, w2) # (n, d_model)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda8ea8",
   "metadata": {},
   "source": [
    "### Problem 2.6: Test your MLP implementation\n",
    "\n",
    "To test if you have the correct MLP implementation, you can run the following \n",
    "test line. We show the expected output if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c029d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
       "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
       "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77c039",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e10058",
   "metadata": {},
   "source": [
    "### Final Prediction Layer\n",
    "\n",
    "A Transformer model iteratively applies multi-head attention and MLP layers to process the input. This produces a processed representation of shape $n \\times d_{model}$. To make the final prediction (e.g., predict the next token), we need to map the $d_{model}$-dimensional embedding vectors to logits over the output vocabulary. To do this, we simply apply a linear map that maps from $d_{model}$ to $\\mathtt{vocab\\_size}$.\n",
    "\n",
    "The starter code for this is given below; you need to complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96ed10",
   "metadata": {},
   "source": [
    "### Problem 2.7: Implement the prediction layer as logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "682c8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_head(x, params):\n",
    "    # get needed parameters\n",
    "    w = params['fc_out.weight'].T # (d_model, vocab_size)\n",
    "    b = params['fc_out.bias'] # (vocab_size,)\n",
    "\n",
    "    # Your code here\n",
    "    logits = np.dot(x, w) + b # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4fa34",
   "metadata": {},
   "source": [
    "### Problem 2.8: Test the prediction head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc2486eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
       "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
       "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_head(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703eaab",
   "metadata": {},
   "source": [
    "```\n",
    "array([[ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656f53",
   "metadata": {},
   "source": [
    "### Putting it all together: A Full Transformer Language Model\n",
    "\n",
    "We are now ready to put this all together to assemble our Transformer Language Model. Recall that the Transformer architecture consists of iteratively applying multi-head attention and MLPs. Each time we apply attention or the MLP, we also apply a *residual connection*: $X^{(\\ell + 1)} = X^{(\\ell)} + F(X^{(\\ell)})$. This can be interpreted as a mechanism to enable easy communication between different layers (some people call refer to this idea as the \"residual stream\"). Real Transformers also include layer normalization in each layer, but we omit this for simplicity in this problem.\n",
    "\n",
    "The full algorithm is given below:\n",
    "1. Embed the tokens using the embedding lookup table: $(t_1, ..., t_n) \\mapsto (E_{t_1}, ..., E_{t_n}) =: X^{(0)}$\n",
    "2. Add the positional embeddings: $X^{(0)} \\gets X^{(0)} + (PE_1, ..., PE_n)$\n",
    "3. For each layer $\\ell = 1, ..., L$:\n",
    "    1. Apply Multi-Head Attention: $\\tilde{X}^{(\\ell)} \\gets X^{(\\ell-1)} + \\mathrm{MultiHeadAttention}(X^{(\\ell-1)})$.\n",
    "    2. Apply the MLP: $X^{(\\ell)} \\gets \\tilde{X}^{(\\ell)} + \\mathrm{MLP}(\\tilde{X}^{(\\ell)})$.\n",
    "4. Compute the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56a03f",
   "metadata": {},
   "source": [
    "### Problem 2.9: Complete the implementation\n",
    "\n",
    "Complete the starter code below, which takes embeddings, adds positional encoding, \n",
    "and then adds the attention and MLP components to each layer. Remember that \n",
    "everything is added together, with the computations in one layer added to the outputs of the previous layer, forming the \"residual stream\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79983203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(tokens, params):\n",
    "    # tokens: (n,) integer array\n",
    "    # params: dictionary of parameters\n",
    "\n",
    "    # map tokens to embeddings using embed_tokens\n",
    "    x = embed_tokens(tokens, params)  # (n, d_model)\n",
    "\n",
    "    # add positional embeddings\n",
    "    pe = get_sinusoidal_positional_embeddings(x.shape[0], x.shape[1]) # (n, d_model)\n",
    "    # your code here\n",
    "    x = x + pe\n",
    "\n",
    "    # transformer blocks\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        # compute multi-head self-attention and add residual\n",
    "        # your code here\n",
    "        attn_out = multi_head_attention(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = x + attn_out\n",
    "\n",
    "        # compute MLP and add residual\n",
    "        mlp_out = mlp(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = x + mlp_out\n",
    "\n",
    "    # compute logits via the prediction_head\n",
    "    # your code here\n",
    "    logits = prediction_head(x, params) # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9fb54",
   "metadata": {},
   "source": [
    "### Problem 2.10: Test your implementation\n",
    "\n",
    "You can check your implementation against the expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2a13d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9641349 , -5.12566872, -5.90677718, -5.57889839, -3.85043564],\n",
       "       [-2.31297379, -4.9703405 , -3.49086668, -5.3996587 , -3.99684942],\n",
       "       [-2.97001726, -4.84049568, -4.04949194, -4.01892107, -6.03805991]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer([0, 1, 2], params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3af6d",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "```\n",
    "array([[-1.96413494, -5.12566872, -5.90677725, -5.57889829, -3.85043572],\n",
    "       [-2.31297382, -4.97034061, -3.49086669, -5.39965866, -3.99684957],\n",
    "       [-2.97001738, -4.8404957 , -4.04949199, -4.01892112, -6.03806011]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140e28",
   "metadata": {},
   "source": [
    "### Generate some text\n",
    "\n",
    "Below, we provide some code for generating text from a Transformer language model. The sampling procedure is *autoregressive*. This means that we input some text to the model and it outputs a distribution over next tokens. We sample the next token and append it to the text, then repeat the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b513e",
   "metadata": {},
   "source": [
    "### Problem 2.11: Complete the next token generator\n",
    "\n",
    "Complete the next token generator, but filling in the missing code below. This uses \"temperature\" to focus on the more probable tokens in a given context (as the temperature decreases). This results in sampling according to \n",
    "$ \\mathrm{Softmax}(\\mathrm{logits}/T)$ where $T \\geq 0$ is the temperature; lower temperature places higher probability on tokens having larger logits. Greedy sampling corresponds to $T=0$, and selects the token with the largest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62cc0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_transformer(prefix_text, params, max_len=128, greedy=False, temperature=0.9):\n",
    "    # encode seed text\n",
    "    prefix_tokens = list(enc.encode(prefix_text))\n",
    "\n",
    "    # initialize generated tokens\n",
    "    generated_tokens = prefix_tokens\n",
    "\n",
    "    # generate new tokens\n",
    "    for i in range(max_len):\n",
    "        # predict next token\n",
    "        logits = transformer(generated_tokens, params)\n",
    "        # logits[-1] corresponds to prediction of the next token\n",
    "        next_logits = logits[-1]\n",
    "        \n",
    "        if greedy:\n",
    "            # Your code here\n",
    "            next_token = np.argmax(next_logits)\n",
    "        else:\n",
    "            # Your code here\n",
    "            scaled_logits = next_logits / temperature\n",
    "            probabilities = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits))\n",
    "            next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        # add next token to generated tokens\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "    # This converts the tokens to text, using the tiktoken decoder:\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3651d",
   "metadata": {},
   "source": [
    "### Problem 2.12: Test your implementation by generating text. You're the Bard!\n",
    "\n",
    "Use your implementation to generate text according to the model. Generate text at different temperatures. Do the results make sense? Comment on the quality of the model. What changes to the model would lead to better results? Comment in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba145d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "\n",
      "First Citizen:\n",
      "We cannot\n",
      "\n",
      "First Citizen:\n",
      "He that hath done fell, sir, sir, sir, sir, sir, sir, sir, sir,\n",
      "Which ne'er could the belly answer'd--\n",
      "Which you,\n",
      "Which you shall fell'd:\n",
      "Which you shall tell you'll hear it smile, Iliber,\n",
      "Which ne'--it it belly taunted head the belly,\n",
      "MENENIUS:\n",
      "There answer'd, you'll have lusedup\n",
      "'\n",
      "There\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=True, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdbfc520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "\n",
      "\n",
      "First Citizen:\n",
      "MENENIUS:\n",
      "The sen, what senenenators of Rome are this good belly,\n",
      "First Citizen:\n",
      "The countrymenenenenebly like a gen though so!\n",
      "Which the belly,\n",
      "Sir counsed it did mute, the midst o' the body.\n",
      "\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "Sselly's answer?\n",
      "Our bashould by the belly,\n",
      "Sir, I shall tell you\n",
      "First C\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=False, temperature=0.8, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d137b6ff-7f40-4bd7-b1d1-666afb0531de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "\n",
      "First Citizen:\n",
      "We cannot\n",
      "First Citizen:\n",
      "MENENIUS:\n",
      "The senators of Rome are this good belly,\n",
      "And you the mutinous members; for examine\n",
      "Their counsels and their cares, digest things rightly\n",
      "Touching the weal o' the common, you shall find\n",
      "No public benefit which you receive\n",
      "But it proceeds or comes from them to you\n",
      "And no way from them to you\n",
      "And no way from them to you\n",
      "And no way from them to you do you do you do you\n",
      "And no way from them to food at the greatness that they live: but the commons or comes from them to you\n",
      "And no way from them asse courty toe of this asse will flour with being one o' What then? What do you do you think, the great to you care fort them as you do you could you\n",
      "And no subling the commons or comes from them to you could yieve\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=False, temperature=0.1, max_len=250)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87916843-5d1e-4d5c-bfa3-efea350bee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "Pats, we'll hear it side o' the common\n",
      "First Citizen:\n",
      "Pating here?\n",
      "First Citizen:\n",
      "Their countrymor heart could belly, I shall famine\n",
      "\n",
      "They answer a timeed head, I shall famselly, I shall fell'd--\n",
      "'-- taunting the leg, famine\n",
      "MENENIUS:\n",
      "Sir, what smile,\n",
      "As well as speak--of the belly's answer could by the muthould you receive that\n",
      "Which ne'From me do deed his nat o'-- that delieds and thus-- muth like a luselly's answeringly replile and thus--\n",
      "Or belly's answer came from in hand?\n",
      "There I will vigestame from thus-- mutinous partic body's answer came from the belly's answer came from the belly particior ice did remain we might gain\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=False, temperature=0.8, max_len=250)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff574506",
   "metadata": {},
   "source": [
    "### Problem 2.12\n",
    "The results demonstrate that the model performs reasonably well in generating Shakespearean text. Adjusting the temperature parameter shows that lower values lead to outputs resembling greedy decoding, where the model consistently predicts tokens like \"sir,\" likely due to its frequent use in Shakespeare's works. Higher temperatures introduce more randomness, resulting in text that feels less Shakespearean and more like the model's unique interpretation. Experimenting with max_len highlights the transformer's ability to generate longer and more coherent text compared to vanilla RNNs, though some incoherence remains.\n",
    "\n",
    "To improve performance, the model could benefit from an increased size, as it currently has only 4 layers, whereas models like BERT and modern LLMs use 12 and upwards of 128 layers, respectively. Expanding the training dataset would also enhance the model’s understanding of context and vocabulary. Finally, fine-tuning the model on downstream tasks with supervised prompts could further improve its ability to generate coherent and contextually appropriate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1443b4b-286a-4c76-95c2-5de406953115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sds365",
   "language": "python",
   "name": "sds365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
