{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c202c768",
   "metadata": {
    "id": "5ccb1d8a"
   },
   "source": [
    "# Intermediate Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 5 is due Wednesday, December 6 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * RNNs and GRUs\n",
    " * Transformers\n",
    "\n",
    "This assignment will also help to solidify your Python skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4fcc69",
   "metadata": {
    "id": "78f64b66"
   },
   "source": [
    "## Problem 1: Elephants Can Remember (25 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ce512",
   "metadata": {
    "id": "ba3663cd"
   },
   "source": [
    "![ECR](https://upload.wikimedia.org/wikipedia/en/e/e3/Elephants_can_Remember_First_Edition_Cover_1972.jpg) \n",
    "\n",
    "In this problem, we will work with \"vanilla\" Recurrent Neural Networks (RNNs) and Recurrent Neural Networks with Gated Recurrent Units (GRUs). The models in this part of the assignment will be character-based models, trained on an extract of the book [Elephants Can Remember](https://en.wikipedia.org/wiki/Elephants_Can_Remember) by Agatha Christie. To reduce the size of our vocabulary, the text is pre-processed by converting the letters to lower case and removing numbers. The code below shows some information about our training and test set. All the necessary files for this problem are available through Canvas, under the file name \"problem1_data\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f2320",
   "metadata": {
    "id": "0804721c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import random\n",
    "  \n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13f00f8",
   "metadata": {
    "id": "f93de082"
   },
   "outputs": [],
   "source": [
    "with open('problem1_data/Agatha_Christie_train.txt', 'r') as file:\n",
    "    train_text = file.read()\n",
    "    \n",
    "with open('problem1_data/Agatha_Christie_test.txt', 'r') as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "vocabulary = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Dictionaries to go from a character to index and vice versa\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cc38b",
   "metadata": {
    "id": "7320ed64"
   },
   "outputs": [],
   "source": [
    "# The first 500 characters of our training set\n",
    "train_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898aa025",
   "metadata": {
    "id": "3a50abac"
   },
   "outputs": [],
   "source": [
    "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
    "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
    "print(\"The test set contains\", len(test_text) ,\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9babdf",
   "metadata": {
    "id": "d02ce399"
   },
   "source": [
    "### Problem 1.1: The Diversity of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd4d6a",
   "metadata": {
    "id": "6951a417"
   },
   "source": [
    "Before jumping into coding, let's start with comparing the language models we will be using in this assigment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef5175",
   "metadata": {
    "id": "472741ad"
   },
   "source": [
    "1. Describe the differences between a Vanilla RNN and a GRU network. In your explanation, make sure you mention the issues with vanilla RNNs and how GRUs try to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60ee93",
   "metadata": {
    "id": "06832930"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d2ebc",
   "metadata": {
    "id": "61c45e86"
   },
   "source": [
    "2. Describe at least two advantages of a character based language model over a word based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a6254",
   "metadata": {
    "id": "8b07db8a"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1d00d",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Problem 1.2: Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d9484",
   "metadata": {
    "id": "85bdca8b"
   },
   "source": [
    "The code below loads in a pretrained vanilla RNN model with two layers. The model is set up exactly like in the lecture slides (with tanh activation layers in the recurrent layers) with the addition of biases (intercepts) in every layer (i.e. the recurrent layer and the dense layer). The training process consisted of 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81616ee7",
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"problem1_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edb0cb",
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6d90d",
   "metadata": {
    "id": "a4bcccd8"
   },
   "source": [
    "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7992aa",
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd298",
   "metadata": {
    "id": "cfc7420f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f5956",
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a7fbc",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Problem 1.3: Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c409f",
   "metadata": {
    "id": "1dd93ee7"
   },
   "source": [
    "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1331cf0",
   "metadata": {
    "id": "82291909"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"problem1_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cd531",
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb62e",
   "metadata": {
    "id": "cad5b34f"
   },
   "source": [
    "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
    "\n",
    "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022548e",
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83980cb",
   "metadata": {
    "id": "117fdc8f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff326aa8",
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5700e",
   "metadata": {
    "id": "6a35ba68"
   },
   "source": [
    "### Problem 1.4: Can Elephants Remember Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9785",
   "metadata": {
    "id": "09cd85c4"
   },
   "source": [
    "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
    "$$\n",
    "\n",
    "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
    "\n",
    "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
    "\n",
    "$$\n",
    "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\} \n",
    "$$\n",
    "\n",
    "Don't forget to go back to the normal perplexity after this transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d5017",
   "metadata": {
    "id": "709c6d4e"
   },
   "source": [
    "1. Before calculating the perplexity of a test sequence, start with comparing the outputs of 2.2 and 2.3. Do you see any differences in the generated text of the Vanilla RNN model and the GRU model? Rerun your functions a couple of times (because of stochasticity) and use different prompts. Briefly discuss why you would expect (or not expect) certain differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a25e4e28",
   "metadata": {
    "id": "3ccf8083"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de90df",
   "metadata": {
    "id": "5c9a5bf5"
   },
   "source": [
    "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca5c09",
   "metadata": {
    "id": "e0c2a390"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aef0f7",
   "metadata": {
    "id": "9ab728c9"
   },
   "source": [
    "3. As seen in part 2 and 3 of this problem, the text generation is not perfect. Describe some possible model improvements that could make the quality of the generated text better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d532d",
   "metadata": {
    "id": "8df914d2"
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b7fe6",
   "metadata": {},
   "source": [
    "## Problem 2: Be the Bard (30 points)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Shakespeare.jpg\" alt=\"William\" width=\"200\" style=\"float:left; padding:15px\"/>\n",
    "\n",
    "Transformer models are the current state of the art in many sequence modeling tasks, and the Transformer architecture underlies most Large Language Models (LLMs), including ChatGPT, Llama, Mistral, etc.\n",
    "\n",
    "In this problem, we will implement a Transformer language model from scratch in `numpy`. You will be provided with the weights of a small, slightly simplified Transformer language model that we trained on the works of Shakespeare. We will walk through implementing each component of the Transformer architecture and ultimately assemble this into a language model that can generate some text in the style of the Bard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93cca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#!pip install tiktoken\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8019aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "vocab_size = 1024\n",
    "block_size = 128\n",
    "\n",
    "transformer_model_weights = np.load(f'problem2_model_parameters/model_weights_D{d_model}L{n_layers}H{n_heads}.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0401772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "    word_embedding.weight\n",
      "    position_embedding.inv_freq\n",
      "    layers.0.attention.wq.weight\n",
      "    layers.0.attention.wk.weight\n",
      "    layers.0.attention.wv.weight\n",
      "    layers.0.attention.wo.weight\n",
      "    layers.0.feed_forward.0.weight\n",
      "    layers.0.feed_forward.2.weight\n",
      "    layers.1.attention.wq.weight\n",
      "    layers.1.attention.wk.weight\n",
      "    layers.1.attention.wv.weight\n",
      "    layers.1.attention.wo.weight\n",
      "    layers.1.feed_forward.0.weight\n",
      "    layers.1.feed_forward.2.weight\n",
      "    layers.2.attention.wq.weight\n",
      "    layers.2.attention.wk.weight\n",
      "    layers.2.attention.wv.weight\n",
      "    layers.2.attention.wo.weight\n",
      "    layers.2.feed_forward.0.weight\n",
      "    layers.2.feed_forward.2.weight\n",
      "    layers.3.attention.wq.weight\n",
      "    layers.3.attention.wk.weight\n",
      "    layers.3.attention.wv.weight\n",
      "    layers.3.attention.wo.weight\n",
      "    layers.3.feed_forward.0.weight\n",
      "    layers.3.feed_forward.2.weight\n",
      "    fc_out.weight\n",
      "    fc_out.bias\n"
     ]
    }
   ],
   "source": [
    "print('Parameters:')\n",
    "for key in transformer_model_weights.keys():\n",
    "    print(f'    {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c04f5",
   "metadata": {},
   "source": [
    "### Problem 2.1: Describe the model parameters\n",
    "\n",
    "Describe the role of the parameters of the model. What are the weights `wq.weight`, `wk.weight`, `wv.weight`, and `wo.weight`? What are the dimensions of these matrices? What about the role and dimensions of the feedforward weights `feed_forward.0.weight` and `feed_forward.2.weight`?\n",
    "\n",
    "You can refer to the descriptions below as well as lecture notes. Note, in particular, in the comments preceding Problem 2.4 that the attention matrices across heads are \"packed together\" when you read in the parameters in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66841e",
   "metadata": {},
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c7b84",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To process text with a neural network model, we need to first tokenize it in order to convert it to a numerical format that the model can understand and process. The tokenizer converts strings into sequences of integer tokens in a fixed vocabulary. There are different ways to do this. For this problem, we trained a custom [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  (BPE) tokenizer on Shakespeare text, setting the vocabulary size to 1024. The code below demonstrates how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1705eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BPE tokenizer\n",
    "with open('problem2_model_parameters/bpe1024_enc_full.pkl', 'rb') as pickle_file:\n",
    "    enc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed255879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n",
      "\n",
      "Encoded:\n",
      "[462, 320, 307, 258, 813, 63, 323, 621, 331, 800, 258, 697, 305, 10, 889, 801, 845, 813, 504, 260, 109, 408, 366, 818, 59]\n",
      "\n",
      "Tokens: \n",
      "['What', \"'s\", ' in', ' a', ' name', '?', ' that', ' which', ' we', ' call', ' a', ' ro', 'se', '\\n', 'By', ' any', ' other', ' name', ' would', ' s', 'm', 'ell', ' as', ' sweet', ';']\n",
      "\n",
      "Decoded text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n"
     ]
    }
   ],
   "source": [
    "# text to tokenize\n",
    "text = \"\"\"What's in a name? that which we call a rose\n",
    "By any other name would smell as sweet;\"\"\"\n",
    "\n",
    "print('Original text:')\n",
    "print(text)\n",
    "encoded = enc.encode(text)\n",
    "print()\n",
    "\n",
    "print('Encoded:')\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Tokens: ')\n",
    "print([enc.decode([idx]) for idx in encoded])\n",
    "decoded = enc.decode(encoded)\n",
    "print()\n",
    "\n",
    "print('Decoded text:')\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536a8cd",
   "metadata": {},
   "source": [
    "### Problem 2.2a: Token Embeddings\n",
    "\n",
    "After tokenization, we get a sequence of integers that represent the text to processed, with each integer index corresponding to a particular token in the vocabulary (e.g., a word or word-part). The first step in processing this text is to turn it into a vector representation. This is done via a learned embedding look-up table. For each token in the vocabulary $t \\in \\mathcal{V}$, we learn an embedding $E_t \\in {\\mathbb R}^d$. A sequence of tokens $(t_1, ..., t_n)$ is transformed to a vector representation by mapping each token to its embedding $(E_{t_1}, ..., E_{t_n}) \\in {\\mathbb R}^{n \\times d}$. This sequence of vectors is what the neural network model ultimately operates over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3c9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens, params):\n",
    "    \"\"\"\n",
    "    Embed tokens using the input embeddings.\n",
    "\n",
    "    Args:\n",
    "        tokens (np.array): array of token indices, shape (n_tokens,)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # get needed parameters\n",
    "    embeddings = params['word_embedding.weight'] # shape (vocab_size, d_model)\n",
    "    # embeddings is a look up table for embeddings, with rows corresponding to token indices\n",
    "    # i.e., embeddings[token_index] returns the embedding for the token with index token_index\n",
    "\n",
    "    # look up embeddings\n",
    "    ...\n",
    "\n",
    "    return embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc75af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
       "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
       "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens(np.array([1, 2, 3]), params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b3e99",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
    "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
    "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
    "      dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4c27",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Transformer models are by-default permutation-equivariant. That is, they don't understand order or position. To make them understand positional information, we need to encode it directly in the token embeddings. One way to do this is to represent each possible position $i$ with its own position embedding ${PE}_i \\in \\mathbb{R}^{d}$. One way to encode the position of each token is to simply add a positional embedding representing the position of the token.\n",
    "\n",
    "In the original Transformer paper, the authors propose a particular choice for ${PE}_i \\in {\\mathbb R}^{d}$ based on sines and cosines with frequencies depending on the position $i$. Since the original proposal, many follow-up works proposed different positional encoding methods aiming to improve performance and length-generalization. In this problem, we'll use the sinusoidal positional encodings of the original Transformer paper.\n",
    "\n",
    "We provide the code for computing these sinusoidal positional embeddings below. To give some intuition about the structure of the sinusoidal positional embeddings, we also plot a heatmap of the pairwise inner products $\\langle PE_i, PE_j \\rangle$. We see that that positions that are closer together have more similar positional embeddings, with additional oscillatory behavior on top of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215faef",
   "metadata": {},
   "source": [
    "### Problem 2.2b: Describe the positional embeddings\n",
    "\n",
    "Below is an implementation of the positional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f9aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embeddings(sequence_length, dim, base=10000):\n",
    "    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))\n",
    "    t = np.arange(sequence_length)\n",
    "    sinusoid_inp = np.einsum(\"i,j->ij\", t, inv_freq)\n",
    "\n",
    "    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n",
    "\n",
    "    emb = np.concatenate((sin, cos), axis=-1)\n",
    "    # return emb[None, :, :]\n",
    "    return emb[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4975426",
   "metadata": {},
   "source": [
    "Explore the positional embeddings by computing the inner-product between all pairs of positional embeddings, and then plotting the similarity matrix as a heat map. Do the results make sense? What are the positional embeddings designed to model? Do they do this effectively? Comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = get_sinusoidal_positional_embeddings(128, 256)\n",
    "\n",
    "#Your code here\n",
    "pe_similarity = ...\n",
    "\n",
    "sns.heatmap(pe_similarity)\n",
    "plt.title('Similarity Structure of Sinusoidal Positional Embeddings')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92850d0c",
   "metadata": {},
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc783c84",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The core operation in a Transformer is multi-head attention, sometimes called self-attention. In this problem, we will implement multi-head attention in numpy from scratch, given the trained parameters of the model.\n",
    "\n",
    "The input is a sequence of vectors $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$. For each attention head $h \\in [n_h]$, the parameters consist of a query projection matrix $W_q^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, a key projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, and a value projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$. Here, $d_h$ is the \"head dimension\", taken to be $d_{model} / n_h$ (to maintain the same dimensionality between the input and output). The algorithm, for each head, is the following:\n",
    "1. Compute the queries $Q = X W_q^h$, keys $K = X W_k^h$, and values $V = X W_v^h$. Note that the linear maps are applied independently for each token across the embedding dimension (not sequence dimension), such that $Q, K, V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "2. Compare the queries and keys via inner products to get an $n \\times n$ attention matrix $A = \\mathrm{Softmax}(Q K^{\\intercal} / \\sqrt{d_h}) \\in {\\mathbb R}^{n \\times n}$.\n",
    "3. Use the attention scores $A$ to select values, producing the output of the self-attention head: $\\mathrm{head}_h = A V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "We then concatenate the retrieved values across all heads, and apply a final linear map. Putting this all together yields:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathrm{head}_h &= \\mathrm{Softmax}((X W_q^h) (X W_k^h)^{\\intercal}/ \\sqrt{d_h}) X W_v^h\\\\\n",
    "    \\mathrm{MultiHeadAttention}(X) &= \\mathrm{concat}(\\mathrm{head}_1, ..., \\mathrm{head}_{n_h}) W_o\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Note that the matrices $W_q^h$, $W_k^h$, $W_v^h$ are \"packed together\" across heads when you read in the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf260",
   "metadata": {},
   "source": [
    "### Problem 2.3: Implement multi-head attention\n",
    "\n",
    "Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33c8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we provide a couple of utility functions\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # a stable implementation of the softmax function\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def apply_presoftmax_causal_mask(attn_scores):\n",
    "    # apply a causal mask to the attention scores (set the entries above the diagonal to -inf)\n",
    "    n = attn_scores.shape[-1]\n",
    "    mask = np.triu(np.ones((n, n)), k=1)\n",
    "    masked_scores = attn_scores - 1e9 * mask\n",
    "    return masked_scores\n",
    "\n",
    "def multi_head_attention(x, params, layer_prefix='layers.0'):\n",
    "    \"\"\"\n",
    "    Compute multi-head self-attention.\n",
    "\n",
    "    Args:\n",
    "        x (np.array): input tensor, shape (n, d_model)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "        layer_prefix (str): prefix of parameter names corresponding to the layer\n",
    "        verbose (bool): whether to print intermediate shapes\n",
    "    \"\"\"\n",
    "\n",
    "    # get parameters of multi-head attention layer\n",
    "    wq = params[f'{layer_prefix}.attention.wq.weight'].T # (d_model, d_model)\n",
    "    wk = params[f'{layer_prefix}.attention.wk.weight'].T # (d_model, d_model)\n",
    "    wv = params[f'{layer_prefix}.attention.wv.weight'].T # (d_model, d_model)\n",
    "    wo = params[f'{layer_prefix}.attention.wo.weight'].T # (d_model, d_model)\n",
    "\n",
    "    head_dim = d_model // n_heads # dimension of each head\n",
    "    attn_scale = 1 / math.sqrt(head_dim) # scaling factor for attention scores\n",
    "\n",
    "    # the wq, wk, wv, wo matrices contain weights for all heads, concatenated\n",
    "    # first, we split wq, wk, wv, wo into heads\n",
    "    # note: there are more efficient implementations, but this is more verbose/pedagogical\n",
    "    wq = wq.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wk = wk.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wv = wv.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "\n",
    "    head_outputs = []\n",
    "    for head in range(n_heads):\n",
    "\n",
    "        # get head-specific parameters (these are the query/key/value projections for this head)\n",
    "        wqh = wq[head] # (d_model, head_dim)\n",
    "        wkh = wk[head] # (d_model, head_dim)\n",
    "        wvh = wv[head] # (d_model, head_dim)\n",
    "\n",
    "        # compute queries, keys, values\n",
    "        # your code here\n",
    "        q = ... # (n, head_dim)\n",
    "        k = ... # (n, head_dim)\n",
    "        v = ... # (n, head_dim)\n",
    "\n",
    "        # compute attention scores\n",
    "        # your code here\n",
    "        attn_scores = ... # (n, n)\n",
    "\n",
    "        attn_scores = apply_presoftmax_causal_mask(attn_scores)\n",
    "        attn_scores = ... # (n, n)\n",
    "\n",
    "        # apply attention scores to values\n",
    "        # your code here\n",
    "        head_out = ... # (n, head_dim)\n",
    "\n",
    "        # store the head output\n",
    "        head_outputs.append(head_out)\n",
    "\n",
    "    # concatenate all head outputs\n",
    "    head_outputs = ... # (n, d_model)\n",
    "\n",
    "    # apply output linear map W_o to concatenated head outputs\n",
    "    # your code here\n",
    "    output = ...  # (n, d_model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f2811",
   "metadata": {},
   "source": [
    "### Problem 2.4: Test your Attention implementation\n",
    "\n",
    "To test if you have the correct implementation, you can run the following \n",
    "test line. We show the expected output if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_attention(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e60b",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262bb79",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Each Transformer layer (i.e., block) consists of two operations: 1) (multi-head) self-attention, which enables exchange of information between tokens, and 2) a multi-layer perceptron, which processes each token independently. A Transformer model is essentially just alternating between these two operations. In this problem, we will implement the multi-layer perceptron step. Typically, the MLP at each layer is simply a two-layer (one hidden layer) MLP or Feed Forward Network. In our model, we use a ReLU activation in the hidden layer, though other activations are possible. The same MLP network is applied to each token embedding in the sequence independently. \n",
    "\n",
    "Given $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$, we apply the MLP as follows:\n",
    "\n",
    "$$\\mathrm{MLP}(X) = \\mathrm{ReLU}(X W_1) W_2$$\n",
    "\n",
    "Note that we don't use biases for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e95c1",
   "metadata": {},
   "source": [
    "### Problem 2.5: Implement the MLP\n",
    "\n",
    "Next, we need to apply the multi-layer perceptron in each layer. Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bab44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def mlp(x, params, layer_prefix='layers.0'):\n",
    "    # get MLP parameters\n",
    "    w1 = params[f'{layer_prefix}.feed_forward.0.weight'].T # (d_model, d_ff)\n",
    "    w2 = params[f'{layer_prefix}.feed_forward.2.weight'].T # (d_ff, d_model)\n",
    "\n",
    "    # Your code here \n",
    "    o = ... # (n, d_model)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda8ea8",
   "metadata": {},
   "source": [
    "### Problem 2.6: Test your MLP implementation\n",
    "\n",
    "To test if you have the correct MLP implementation, you can run the following \n",
    "test line. We show the expected output if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c029d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77c039",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e10058",
   "metadata": {},
   "source": [
    "### Final Prediction Layer\n",
    "\n",
    "A Transformer model iteratively applies multi-head attention and MLP layers to process the input. This produces a processed representation of shape $n \\times d_{model}$. To make the final prediction (e.g., predict the next token), we need to map the $d_{model}$-dimensional embedding vectors to logits over the output vocabulary. To do this, we simply apply a linear map that maps from $d_{model}$ to $\\mathtt{vocab\\_size}$.\n",
    "\n",
    "The starter code for this is given below; you need to complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96ed10",
   "metadata": {},
   "source": [
    "### Problem 2.7: Implement the prediction layer as logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "682c8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_head(x, params):\n",
    "    # get needed parameters\n",
    "    w = params['fc_out.weight'].T # (d_model, vocab_size)\n",
    "    b = params['fc_out.bias'] # (vocab_size,)\n",
    "\n",
    "    # Your code here\n",
    "    logits = ... # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4fa34",
   "metadata": {},
   "source": [
    "### Problem 2.8: Test the prediction head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2486eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_head(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703eaab",
   "metadata": {},
   "source": [
    "```\n",
    "array([[ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656f53",
   "metadata": {},
   "source": [
    "### Putting it all together: A Full Transformer Language Model\n",
    "\n",
    "We are now ready to put this all together to assemble our Transformer Language Model. Recall that the Transformer architecture consists of iteratively applying multi-head attention and MLPs. Each time we apply attention or the MLP, we also apply a *residual connection*: $X^{(\\ell + 1)} = X^{(\\ell)} + F(X^{(\\ell)})$. This can be interpreted as a mechanism to enable easy communication between different layers (some people call refer to this idea as the \"residual stream\"). Real Transformers also include layer normalization in each layer, but we omit this for simplicity in this problem.\n",
    "\n",
    "The full algorithm is given below:\n",
    "1. Embed the tokens using the embedding lookup table: $(t_1, ..., t_n) \\mapsto (E_{t_1}, ..., E_{t_n}) =: X^{(0)}$\n",
    "2. Add the positional embeddings: $X^{(0)} \\gets X^{(0)} + (PE_1, ..., PE_n)$\n",
    "3. For each layer $\\ell = 1, ..., L$:\n",
    "    1. Apply Multi-Head Attention: $\\tilde{X}^{(\\ell)} \\gets X^{(\\ell-1)} + \\mathrm{MultiHeadAttention}(X^{(\\ell-1)})$.\n",
    "    2. Apply the MLP: $X^{(\\ell)} \\gets \\tilde{X}^{(\\ell)} + \\mathrm{MLP}(\\tilde{X}^{(\\ell)})$.\n",
    "4. Compute the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56a03f",
   "metadata": {},
   "source": [
    "### Problem 2.9: Complete the implementation\n",
    "\n",
    "Complete the starter code below, which takes embeddings, adds positional encoding, \n",
    "and then adds the attention and MLP components to each layer. Remember that \n",
    "everything is added together, with the computations in one layer added to the outputs of the previous layer, forming the \"residual stream\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79983203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(tokens, params):\n",
    "    # tokens: (n,) integer array\n",
    "    # params: dictionary of parameters\n",
    "\n",
    "    # map tokens to embeddings using embed_tokens\n",
    "    x = ...  # (n, d_model)\n",
    "\n",
    "    # add positional embeddings\n",
    "    pe = get_sinusoidal_positional_embeddings(x.shape[0], x.shape[1]) # (n, d_model)\n",
    "    # your code here\n",
    "    x = ...\n",
    "\n",
    "    # transformer blocks\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        # compute multi-head self-attention and add residual\n",
    "        # your code here\n",
    "        attn_out = multi_head_attention(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = ...\n",
    "\n",
    "        # compute MLP and add residual\n",
    "        mlp_out = mlp(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = ...\n",
    "\n",
    "    # compute logits via the prediction_head\n",
    "    # your code here\n",
    "    logits = ... # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9fb54",
   "metadata": {},
   "source": [
    "### Problem 2.10: Test your implementation\n",
    "\n",
    "You can check your implementation against the expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a13d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer([0, 1, 2], params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3af6d",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "```\n",
    "array([[-1.96413494, -5.12566872, -5.90677725, -5.57889829, -3.85043572],\n",
    "       [-2.31297382, -4.97034061, -3.49086669, -5.39965866, -3.99684957],\n",
    "       [-2.97001738, -4.8404957 , -4.04949199, -4.01892112, -6.03806011]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140e28",
   "metadata": {},
   "source": [
    "### Generate some text\n",
    "\n",
    "Below, we provide some code for generating text from a Transformer language model. The sampling procedure is *autoregressive*. This means that we input some text to the model and it outputs a distribution over next tokens. We sample the next token and append it to the text, then repeat the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b513e",
   "metadata": {},
   "source": [
    "### Problem 2.11: Complete the next token generator\n",
    "\n",
    "Complete the next token generator, but filling in the missing code below. This uses \"temperature\" to focus on the more probable tokens in a given context (as the temperature decreases). This results in sampling according to \n",
    "$ \\mathrm{Softmax}(\\mathrm{logits}/T)$ where $T \\geq 0$ is the temperature; lower temperature places higher probability on tokens having larger logits. Greedy sampling corresponds to $T=0$, and selects the token with the largest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cc0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_transformer(prefix_text, params, max_len=128, greedy=False, temperature=0.9):\n",
    "    # encode seed text\n",
    "    prefix_tokens = list(enc.encode(prefix_text))\n",
    "\n",
    "    # initialize generated tokens\n",
    "    generated_tokens = prefix_tokens\n",
    "\n",
    "    # generate new tokens\n",
    "    for i in range(max_len):\n",
    "        # predict next token\n",
    "        logits = transformer(generated_tokens, params)\n",
    "        # logits[-1] corresponds to prediction of the next token\n",
    "        if greedy:\n",
    "            # Your code here\n",
    "            next_token = ...\n",
    "        else:\n",
    "            # Your code here\n",
    "            next_token = ...\n",
    "\n",
    "        # add next token to generated tokens\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "    # This converts the tokens to text, using the tiktoken decoder:\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3651d",
   "metadata": {},
   "source": [
    "### Problem 2.12: Test your implementation by generating text. You're the Bard!\n",
    "\n",
    "Use your implementation to generate text according to the model. Generate text at different temperatures. Do the results make sense? Comment on the quality of the model. What changes to the model would lead to better results? Comment in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba145d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=True, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
    "    greedy=False, temperature=0.8, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff574506",
   "metadata": {},
   "source": [
    "### Problem 2.12\n",
    "\n",
    "Your markdown here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
